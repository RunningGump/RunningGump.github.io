<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo博客迁移到一台新电脑]]></title>
    <url>%2F2018%2F10%2F29%2FHexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%8F%A6%E4%B8%80%E5%8F%B0%E7%94%B5%E8%84%91%2F</url>
    <content type="text"><![CDATA[前言由于10月份换了一台Macbook Pro，导致自己搭建的Hexo博客一直停滞了。我想人做事一定要善始善终，不要忘记当初搭建Hexo博客的初心。于是乎，就有了这一篇文章。 迁移思路在已经推送到Github上的Hexo静态网页创建一个分支，利用这个分支来管理自己的Hexo环境文件。 迁移步骤1.在旧机器上克隆Github上面生成的静态文件到本地1git clone https://github.com/username/username.github.io.git 2.针对克隆到本地的文件中，将除去.git文件的所有文件都删除3.将旧机器中所有文件(.gitignore文件中包含的文件除外）拷贝到我们克隆下来的文件内4. 创建并切换到一个叫hexo的分支1git checkout -b hexo 5. 提交复制过来的文件到暂存取1git add -A 6.提交1git commit -m "ceate a new branch file" 7.推送到Github1git push --set-upstream origin hexo 这个时候hexo分支已经创建完毕，接下来，我们在新电脑上搭建环境。 8.新电脑配置环境安装node.js，根据自己电脑系统自行百度安装。 安装git，git相关教程推荐廖雪峰老师的git教程。 安装hexo： npm install -g hexo-cli 9.clone远程仓库到本地1git clone -b hexo https://github.com/username/username.github.io.git 10.根据package.json安装依赖1npm install *** --save 将***替换为package.json文件内的依赖包 11.开始写文章我们现在可以通过hexo n &quot;文章标题&quot; 创建一篇文章了！ 12. 提交hexo环境文件git add . git commit -m &quot;some description&quot; git push origin hexo 13.发布文章1hexo g -d 到这里，我们的Hexo博客就迁移完毕了！！以后再写文章时，只需要重复步骤11、12、13就ok啦！！]]></content>
      <categories>
        <category>“Hexo”</category>
      </categories>
      <tags>
        <tag>Hexo迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Scrapy框架的CrawlSpider类爬取当当全网商品信息]]></title>
    <url>%2F2018%2F09%2F01%2F%E5%9F%BA%E4%BA%8EScrapy%E6%A1%86%E6%9E%B6%E7%9A%84CrawlSpider%E7%B1%BB%E7%88%AC%E5%8F%96%E5%BD%93%E5%BD%93%E5%85%A8%E7%BD%91%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言本项目通过使用Scrapy框架的CrawlSpider类，对当当全网商品信息进行爬取并将信息保存至mysql数据库，当当网反爬措施是对IP访问频率的限制，所以本项目使用了中间件scrapy-rotating-proxies来管控IP代理池，有关代理ip的爬取请见我的另一篇博文。 CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类通过定义一些规则(rule)来跟进所爬取网页中的link，从爬取的网页中获取link并继续爬取。 Github地址: https://github.com/RunningGump/crawl_dangdang 依赖 scrapy 1.5.0 python3.6 mysql 5.7 pymysql 库 scrapy-rotating-proxies 库 fake-useragent 库 创建项目首先，我们需要创建一个Scrapy项目，在shell中使用scrapy startproject命令： 1234567$ scrapy startproject DangdangNew Scrapy project 'Dangdang', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in: /home/geng/DangdangYou can start your first spider with: cd Dangdang scrapy genspider example example.com 创建好一个名为Dangdang的项目后，接下来，你进入新建的项目目录： 1$ cd Dangdang 然后,使用scrapy genspider -t &lt;template&gt; &lt;name&gt; &lt;domain&gt;创建一个spider： 123$ scrapy genspider -t crawl dd dangdang.comCreated spider 'dd' using template 'crawl' in module: Dangdang.spiders.dd 此时，你通过cd ..返回上级目录，使用tree命令查看项目目录下的文件，显示如下： 1234567891011121314151617181920$ cd ..$ tree Dangdang/Dangdang/├── Dangdang│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── __pycache__│ │ ├── __init__.cpython-36.pyc│ │ └── settings.cpython-36.pyc│ ├── settings.py│ └── spiders│ ├── dd.py│ ├── __init__.py│ └── __pycache__│ └── __init__.cpython-36.pyc└── scrapy.cfg4 directories, 11 files 到此为止，我们的项目就创建成功了。 rules在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作设置了爬取规则。 参数介绍： link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。 callback： 回调函数，对link_extractor获得的链接进行处理与解析。 注意事项：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。 follow：是一个布尔(boolean)值，指定了根据规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤链接。 process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request) LinkExtrator参数介绍： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)匹配的URL不提取。 allow_domains：会被提取的链接的域名。 deny_domains：不会被提取链接的域名。 restrict_xpaths：使用Xpath表达式与allow共同作用提取出同时符合对应Xpath表达式和正则表达式的链接； 项目代码编写item.py文件123456789101112import scrapyclass DangdangItem(scrapy.Item): category = scrapy.Field() # 商品类别 title = scrapy.Field() # 商品名称 link = scrapy.Field() # 商品链接 price = scrapy.Field() # 商品价格 comment = scrapy.Field() # 商品评论数 rate = scrapy.Field() # 商品的好评率 source = scrapy.Field() # 商品的来源地 detail = scrapy.Field() # 商品详情 img_link = scrapy.Field() # 商品图片链接 编写pipeline.py文件需提前创建好数据库，本项目创建的数据库名字为dd。 123456789101112131415161718192021222324252627282930313233343536373839import pymysqlfrom scrapy.conf import settings## pipeline默认是不开启的，需在settings.py中开启class DangdangPipeline(object): def process_item(self, item, spider): ##建立数据库连接 conn = pymysql.connect(host="localhost",user="root",passwd='yourpasswd',db="dd",use_unicode=True, charset="utf8") cur = conn.cursor() # 用来获得python执行Mysql命令的方法,也就是我们所说的操作游标 print("mysql connect success") # 测试语句，这在程序执行时非常有效的理解程序是否执行到这一步 try: category = item["category"] title = item["title"] if len(title)&gt;40: title = title[0:40] + '...' link = item["link"] img_link = item['img_link'] price = item["price"] comment = item["comment"] rate = item["rate"] source = item["source"] detail = item["detail"] sql = "INSERT INTO goods(category,title,price,comment,rate,source,detail,link,img_link) VALUES ('%s','%s','%s','%s','%s','%s','%s','%s','%s')" % (category,title,price,comment,rate,source,detail,link,img_link) print(sql) except Exception as err: pass try: cur.execute(sql) # 真正执行MySQL语句，即查询TABLE_PARAMS表的数据 print("insert success") # 测试语句 except Exception as err: print(err) conn.rollback() #事务回滚,为了保证数据的有效性将数据恢复到本次操作之前的状态.有时候会存在一个事务包含多个操作，而多个操作又都有顺序，顺序执行操作时，有一个执行失败，则之前操作成功的也会回滚，即未操作的状态 else: conn.commit() #当没有发生异常时，提交事务，避免出现一些不必要的错误 conn.close() #关闭连接 return item #框架要求返回一个item对象 编写middlewares.py本项目添加了RandomUserAgentMiddleWare中间件，用来随机更换UserAgent。在middlewares.py文件的最后面添加如下中间件： 1234567891011121314151617181920from fake_useragent import UserAgentclass RandomUserAgentMiddleWare(object): """ 随机更换User-Agent """ def __init__(self,crawler): super(RandomUserAgentMiddleWare, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get("RANDOM_UA_TYPE", "random") @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua_type(): return getattr(self.ua, self.ua_type) # 取对象 ua 的 ua_type 的这个属性, 相当于 self.ua.self.ua_type request.headers.setdefault('User-Agent', get_ua_type()) 修改settings.py 文件12345678910111213141516171819202122232425262728293031BOT_NAME = 'Dangdang'SPIDER_MODULES = ['Dangdang.spiders']NEWSPIDER_MODULE = 'Dangdang.spiders'# 不遵循robots协议ROBOTSTXT_OBEY = False# 下载延迟设置为0，提高爬取速度DOWNLOAD_DELAY = 0#禁用Cookie(默认情况下启用)COOKIES_ENABLED = False # 启用所需要的下载中间件DOWNLOADER_MIDDLEWARES = &#123; 'rotating_proxies.middlewares.RotatingProxyMiddleware': 610, 'rotating_proxies.middlewares.BanDetectionMiddleware': 620, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'Dangdang.middlewares.RandomUserAgentMiddleWare': 400,&#125;# 代理IP文件路径,此处需改为你自己的路径ROTATING_PROXY_LIST_PATH = '/home/geng/Projects/Dangdang/proxy.txt'# 随机更换UserAgent RANDOM_UA_TYPE = "random"# 开启pipelineITEM_PIPELINES = &#123; 'Dangdang.pipelines.DangdangPipeline': 300,&#125; spider文件(dd.py)编写12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom Dangdang.items import DangdangItemimport reimport urllib.requestimport jsonclass DdSpider(CrawlSpider): name = 'dd' allowed_domains = ['dangdang.com'] start_urls = ['http://category.dangdang.com/'] # 分析网页链接，编写rules规则,提取商品详情页的链接 rules = ( Rule(LinkExtractor(allow=r'/cp\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html$|/pg\d+-cp\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html$', deny=r'/cp98.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html'), follow=True), Rule(LinkExtractor(allow=r'/cid\d+.html$|/pg\d+-cid\d+.html$', deny=r'/cp98.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html'), follow=True), Rule(LinkExtractor(allow=r'product.dangdang.com/\d+.html$', restrict_xpaths=("//p[@class='name']/a")), callback='parse_item', follow=False), # allow与restrict_xpath配合使用,效果很好,可以更精准筛选链接. ) # 解析商品详情页面 def parse_item(self, response): item = DangdangItem() # 实例化item item["category"] = response.xpath('//*[@id="breadcrumb"]/a[1]/b/text()').extract_first()+'&gt;'+response.xpath('//*[@id="breadcrumb"]/a[2]/text()').extract_first()+'&gt;'+response.xpath('//*[@id="breadcrumb"]/a[3]/text()').extract_first() item["title"] = response.xpath("//*[@id='product_info']/div[1]/h1/@title").extract_first() item["detail"] = json.dumps(response.xpath("//*[@id='detail_describe']/ul//li/text()").extract(),ensure_ascii=False) item["link"] = response.url item["img_link"] =json.dumps(response.xpath("//div[@class='img_list']/ul//li/a/@data-imghref").extract()) try: item["price"] = response.xpath("//*[@id='dd-price']/text()").extract()[1].strip() except IndexError as e: item["price"] = response.xpath("//*[@id='dd-price']/text()").extract()[0].strip() item["comment"] = response.xpath("//*[@id='comm_num_down']/text()").extract()[0] try: item["source"] = response.xpath("//*[@id='shop-geo-name']/text()").extract()[0].replace('\xa0至','') except IndexError as e: item["source"] = '当当自营' # 通过抓包分析,提取商品的好评率 goodsid = re.compile('\/(\d+).html').findall(response.url)[0] # 提取url中的商品id # 提取详情页源码中的categoryPath script = response.xpath("/html/body/script[1]/text()").extract()[0] categoryPath = re.compile(r'.*categoryPath":"(.*?)","describeMap').findall(script)[0] # 构造包含好评率包的链接 rate_url = "http://product.dangdang.com/index.php?r=comment%2Flist&amp;productId="+str(goodsid)+"&amp;categoryPath="+str(categoryPath)+"&amp;mainProductId="+str(goodsid) rate_date = urllib.request.urlopen(rate_url).read().decode("utf-8") # 读取包含好评率的包的内容 item["rate"] = re.compile(r'"goodRate":"(.*?)"').findall(rate_date)[0]+'%' # 用正则表达式提取好评率 yield item 使用方法在命令行中执行以下命令开始爬取： 1$ scrapy crawl dd 结果展示]]></content>
      <categories>
        <category>python3网络爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于scrapy框架爬取西刺代理并验证有效性]]></title>
    <url>%2F2018%2F08%2F02%2F%E5%9F%BA%E4%BA%8Escrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E8%A5%BF%E5%88%BA%E4%BB%A3%E7%90%86%E5%B9%B6%E9%AA%8C%E8%AF%81%E6%9C%89%E6%95%88%E6%80%A7%2F</url>
    <content type="text"><![CDATA[前言在爬取网站数据的时候，一些网站会对用户的访问频率进行限制，如果爬取过快会被封ip，而使用代理可防止被封禁。本项目使用scrapy框架对西刺网站进行爬取，并验证爬取代理的有效性，最终将有效的代理输出并存储到json文件中。 Github地址: https://github.com/RunningGump/crawl_xiciproxy 依赖 python3.6 Scrapy 1.5.0 创建项目首先，我们需要创建一个Scrapy项目，在shell中使用scrapy startproject命令： 1234567$ scrapy startproject xiciproxyNew Scrapy project 'xiciproxy', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in: /home/geng/xiciproxyYou can start your first spider with: cd xiciproxy scrapy genspider example example.com 创建好一个名为xiciproject的项目后，接下来，你进入新建的项目目录： 1$ cd xiciproxy 然后,使用scrapy genspider &lt;name&gt; &lt;domain&gt;创建一个spider： 123$ scrapy genspider xici xicidaili.comCreated spider 'xici' using template 'basic' in module: xiciproxy.spiders.xici 此时，你通过cd ..返回上级目录，使用tree命令查看项目目录下的文件，显示如下： 1234567891011121314151617181920$ cd ..$ tree xiciproxyxiciproxy├── scrapy.cfg└── xiciproxy ├── __init__.py ├── items.py ├── middlewares.py ├── pipelines.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── settings.cpython-36.pyc ├── settings.py └── spiders ├── __init__.py ├── __pycache__ │ └── __init__.cpython-36.pyc └── xici.py4 directories, 11 files 到此为止，我们的项目就创建成功了。 分析页面编写爬虫程序之间，首先需要对待爬取的页面进行分析，主流的浏览器中都带有分析页面的工具或插件，这里我们选用Chrome浏览器的开发者工具分析页面。 链接信息在Chrome浏览器中打开页面http://www.xicidaili.com/, 通过点击国内高匿代理和国内普通代理以及进行翻页操作，会发现以下规律： http://www.xicidaili.com/参数1/参数2 参数1中nn代表高匿代理，nt代表普通代理；参数2中1,2,3,4…代表页数。 数据信息爬取网页信息时一般使用高匿代理，高匿代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真是IP是隐藏的，不会认为我们使用了代理。 本部分以爬取高匿代理为例子来分析如何爬取网页的数据信息。在Chrome浏览器中打开页面http://www.xicidaili.com/nn, 并按F12键来打开开发者工具，点击Elements（元素）来查看其HTML代码，会发现每一条代理的信息都包裹在一个tr标签下，如下图所示： 再来单独对一个tr标签进行分析： 123456789101112131415161718192021222324252627&lt;tr class="odd"&gt; &lt;td class="country"&gt;&lt;img src="http://fs.xicidaili.com/images/flag/cn.png" alt="Cn"&gt;&lt;/td&gt; &lt;td&gt;115.198.35.213&lt;/td&gt; &lt;td&gt;6666&lt;/td&gt; &lt;td&gt; &lt;a href="/2018-07-20/zhejiang"&gt;浙江杭州&lt;/a&gt; &lt;/td&gt; &lt;td class="country"&gt;高匿&lt;/td&gt; &lt;td&gt;HTTPS&lt;/td&gt; &lt;td class="country"&gt; &lt;div title="0.144秒" class="bar"&gt; &lt;div class="bar_inner fast" style="width:85%"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td class="country"&gt; &lt;div title="0.028秒" class="bar"&gt; &lt;div class="bar_inner fast" style="width:96%"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;15天&lt;/td&gt; &lt;td&gt;18-08-04 15:33&lt;/td&gt; &lt;/tr&gt; 会发现：IP地址包裹在td[2]标签下，端口port包裹在td[3]标签下，类型（http/https）包裹在td[6]标签下。 程序编写分析完页面后，接下来编写爬虫。本项目主要是对xici.py进行编写，对settings.py仅做了轻微改动。 实现spider即编写xici.py文件，程序如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import scrapyimport json'''scrapy crawl xici -o out.json -a num_pages=50 -a typ=nn其中`out.json`是输出有效代理的json文件，`num_pages`是爬取页数，`typ`表示代理类型，`nn`是高匿代理，`nt`是普通代理。'''class XiCiSpider(scrapy.Spider): # 每一个爬虫的唯一标识 name = 'xici' # 使用-a选项,可以将命令行参数传递给spider的__init__方法 def __init__(self, num_pages=5, typ='nn', *args, **kwargs): num_pages = int(num_pages) self.num_pages = num_pages self.typ = typ # 定义起始爬取点 def start_requests(self): for page in range(1, self.num_pages + 1): url = 'http://www.xicidaili.com/&#123;&#125;/&#123;&#125;'.format(self.typ, page) yield scrapy.Request(url=url) # 解析response返回的网页 def parse(self, response): proxy_list = response.xpath('//table[@id = "ip_list"]/tr[position()&gt;1]') for tr in proxy_list: # 提取代理的 ip, port, scheme(http or https) ip = tr.xpath('td[2]/text()').extract_first() port = tr.xpath('td[3]/text()').extract_first() scheme = tr.xpath('td[6]/text()').extract_first() # 使用爬取到的代理再次发送请求到http(s)://httpbin.org/ip, 验证代理是否可用 url = '%s://httpbin.org/ip' % scheme proxy = '%s://%s:%s' % (scheme, ip, port) meta = &#123; 'proxy': proxy, 'dont_retry': True, 'download_timeout': 5, # 下面的ip字段是传递给check_available方法的信息,方便检测是否可隐藏ip '_proxy_ip':ip, &#125; yield scrapy.Request(url, callback=self.check_available, meta=meta, dont_filter=True) def check_available(self, response): proxy_ip = response.meta['_proxy_ip'] # 判断代理是否具有隐藏IP功能 if proxy_ip == json.loads(response.text)['origin']: yield&#123; 'proxy': response.meta['proxy'] &#125; 修改配置文件 更改USER_AGENT：西刺代理网站会通过识别请求中的user-agent来判断这次请求是真实用户所为还是机器所为。 不遵守robots协议：网站会通过robots协议告诉搜索引擎那些页面可以抓取，哪些不可以抓取，而robots协议大多不允许抓取有价值的信息，所以咱们不遵守。 禁用cookies：如果用不到cookies，就不要让服务器知道你的cookies。 文件settings.py中的改动如下： 123USER_AGENT = &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36&quot;ROBOTSTXT_OBEY = TrueCOOKIES_ENABLED = False 在编写好xici.py和settings.py后，我们的项目就大功告成啦！ 使用方法使用方法就是在命令行中执行以下命令即可： 1$ scrapy crawl xici -o out.json -a num_pages=10 -a typ=nn 其中out.json是最终输出有效代理的json文件，num_pages是爬取页数，typ表示要爬取的代理类型，nn是高匿代理，nt是普通代理。 提示 ：程序在验证代理有效性的过程中，对于无效的代理会抛出超时异常，不要管这些异常，让程序继续执行直到结束。]]></content>
      <categories>
        <category>python3网络爬虫</category>
      </categories>
      <tags>
        <tag>西刺代理</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
