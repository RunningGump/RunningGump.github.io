<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F11%2F06%2Ftest%2F</url>
    <content type="text"><![CDATA[hello world]]></content>
  </entry>
  <entry>
    <title><![CDATA[824. Goat Latin]]></title>
    <url>%2F2018%2F11%2F05%2F824-Goat-Latin%2F</url>
    <content type="text"><![CDATA[题目山羊拉丁文 给定一个由空格分割单词的句子 S。每个单词只包含大写或小写字母。 我们要将句子转换为 “Goat Latin”（一种类似于 猪拉丁文 - Pig Latin 的虚构语言）。 山羊拉丁文的规则如下： 如果单词以元音开头（a, e, i, o, u），在单词后添加&quot;ma&quot;。例如，单词&quot;apple&quot;变为&quot;applema&quot;。 如果单词以辅音字母开头（即非元音字母），移除第一个字符并将它放到末尾，之后再添加&quot;ma&quot;。例如，单词&quot;goat&quot;变为&quot;oatgma&quot;。 根据单词在句子中的索引，在单词最后添加与索引相同数量的字母&#39;a&#39;，索引从1开始。例如，在第一个单词后添加&quot;a&quot;，在第二个单词后添加&quot;aa&quot;，以此类推。 返回将 S 转换为山羊拉丁文后的句子。 示例 1: 12输入: &quot;I speak Goat Latin&quot;输出: &quot;Imaa peaksmaaa oatGmaaaa atinLmaaaaa&quot; 示例 2: 12输入: &quot;The quick brown fox jumped over the lazy dog&quot;输出: &quot;heTmaa uickqmaaa rownbmaaaa oxfmaaaaa umpedjmaaaaaa overmaaaaaaa hetmaaaaaaaa azylmaaaaaaaaa ogdmaaaaaaaaaa&quot; 说明: S 中仅包含大小写字母和空格。单词间有且仅有一个空格。 1 &lt;= S.length &lt;= 150。 思路此题较简单，先将字符串以&#39;&#39;分开，返回一个包含所有单词的列表，然后遍历列表，分两种情况修改单词即可，最后再将拼接后的字符串返回。 代码12345678910111213class Solution: def toGoatLatin(self, S): """ :type S: str :rtype: str """ x = S.split(' ') for i in range(len(x)): if x[i][0].lower() in ['a', 'e', 'i', 'o', 'u']: x[i] = x[i] + 'ma' + 'a' * (i + 1) else: x[i] = x[i][1:] + x[i][0] + 'ma' + 'a' * (i + 1) return ' '.join(x)]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[483. Smallest Good Base]]></title>
    <url>%2F2018%2F11%2F05%2F483-Smallest-Good-Base%2F</url>
    <content type="text"><![CDATA[题目最小好基数 对于给定的整数 n, 如果n的k（k&gt;=2）进制数的所有数位全为1，则称 k（k&gt;=2）是 n 的一个好基数。 以字符串的形式给出 n, 以字符串的形式返回 n 的最小好进制。 示例 1： 123输入：&quot;13&quot;输出：&quot;3&quot;解释：13 的 3 进制是 111。 示例 2： 123输入：&quot;4681&quot;输出：&quot;8&quot;解释：4681 的 8 进制是 11111。 示例 3： 123输入：&quot;1000000000000000000&quot;输出：&quot;999999999999999999&quot;解释：1000000000000000000 的 999999999999999999 进制是 11。 提示： n的取值范围是 [3, 10^18]。 输入总是有效且没有前导 0。 思路一输入给定数为n，我们遍历[2 , n-1] ​ 如果当前遍历值i为n的基数，则找到了最小好基数 注意，n-1一定是n的一个好基数 那么，如何判断i为n的好基数呢？ 若i为n的好基数，由辗转相除法的逆过程可知，n和i一定满足这样一个关系： $ ((( 1 k + 1)k + 1)k + 1)k+1……=n$ ，其中k为基数。 代码一123456789101112class Solution: def smallestGoodBase(self, n): def checkBase(base, n): currentVal = 1 while currentVal &lt; n: currentVal = currentVal * base + 1 return currentVal == n thisNum = int(n) for i in range(2, thisNum): if checkBase(i, thisNum): return str(i) 思路二先利用数学知识对问题进行分析，将问题简化，然后在去用代码实现。 n 为输入的整数，k为基，m为多项式的个数，根据好基数的定义知，n可以展开为如下式子： $n=1+k+k^2+…+k^{m-1} \tag{1}$ 可以明显看出，（1）式中的元素为等比数列。根据等比数列求和公式得： $n=\frac{k^m-1}{k-1} \tag{2}$ 此题中基数k&gt;=2 ，我们要求的是最小的好基数，即求使（1）式成立的最小的k。观察式子可以发现，当n恒定的时候，k越小则m越大，m代表多项式的个数，由于k至少为2，那么（1）式肯定至少有两项，则m&gt;=2。m的最大值是多少呢？当k取最小值2时，m的值最大，即数字n用二进制表示的时候可拆分出的项最多，计算得到m最大为$int(log_2(m+1)) + 1$，所以， $ 2&lt;= m &lt;int(log_2(m+1)) + 1 \tag{3}$ 从（1）式中可以明显看出：$k^{m-1}&lt;n$，将（3）式进行变形可得 $ k&lt;n^{ \frac{1}{m-1} }$。此时会想到，如果$n^{ m-1}&lt;k+1$也成立的话，就会为我们的计算带来很大的方便，因为如果$ k&lt;n^{ \frac{1}{m-1} }&lt;k+1$ 的话，我们对$ n^{ \frac{1}{m-1} }$取整就可以得到k的值。事实上，(4) 式确实是成立的。 $n^{ \frac{1}{m-1} }&lt;k+1 \tag{4}$ 由（4）式可得， $n&lt;(k+1)^{m-1} \tag{5}$ 将（5）右边的式子进行牛顿二项式展开，与（1）式进行比对，可明显看出（5）式是成立的。 代码二123456789101112131415import math class Solution(object): def smallestGoodBase(self, n): """ :type n: str :rtype: str """ num = int(n) thisLen = int(math.log(num,2)) + 1 while thisLen &gt; 2: thisBase = int(num ** (1.0/(thisLen - 1))) if num * (thisBase - 1) == thisBase ** thisLen - 1: return str(thisBase) thisLen -= 1 return str(num - 1) if num * (thisBase - 1) == thisBase ** thisLen - 1:不能写成if num == (base**goodBaseLen - 1) / (base - 1):，因为计算机精度的问题，当输入n很大时，一些测试用例会因为精度问题导致后面的表达式出错。所以，在计算机的运算式中，能用乘法表示就不用除法。]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[925. Long Pressed Name]]></title>
    <url>%2F2018%2F11%2F02%2F925-Long-Pressed-Name%2F</url>
    <content type="text"><![CDATA[题目长按键入 你的朋友正在使用键盘输入他的名字 name。偶尔，在键入字符 c 时，按键可能会被长按，而字符可能被输入 1 次或多次。 你将会检查键盘输入的字符 typed。如果它对应的可能是你的朋友的名字（其中一些字符可能被长按），那么就返回 True。 示例 1： 123输入：name = &quot;alex&quot;, typed = &quot;aaleex&quot;输出：true解释：&apos;alex&apos; 中的 &apos;a&apos; 和 &apos;e&apos; 被长按。 示例 2： 123输入：name = &quot;saeed&quot;, typed = &quot;ssaaedd&quot;输出：false解释：&apos;e&apos; 一定需要被键入两次，但在 typed 的输出中不是这样。 示例 3： 12输入：name = &quot;leelee&quot;, typed = &quot;lleeelee&quot;输出：true 示例 4： 123输入：name = &quot;laiden&quot;, typed = &quot;laiden&quot;输出：true解释：长按名字中的字符并不是必要的。 提示： name.length &lt;= 1000 typed.length &lt;= 1000 name 和 typed 的字符都是小写字母。 思路使用两个指针解决这个问题。 1234a l e xia a l e e xj 若i&lt;name.length() 且name.charAt(i)==typed.charAt(j),则i++;j++。 1234a l e x ia a l e e x j 若j==0 或 typed.charAt(j)!= typed.charAt(j-1)，则返回false。 1234a l e x ia a l e e x j 1234a l e x ia a l e e x j ….. ….. 1234a l e x ia a l e e x j 注意：&amp;&amp;的运算符，运算顺序从左至右。若&amp;&amp;左边的表达式为false，则不进行&amp;&amp;后面的运算，这个逻辑表达式返回false。 代码12345678910111213141516class Solution &#123; public boolean isLongPressedName(String name, String typed) &#123; int i = 0; int typed_len = typed.length(); for(int j=0; j&lt;typed_len; j++)&#123; if(i&lt;name.length() &amp;&amp; name.charAt(i)==typed.charAt(j))&#123; i++; &#125; else&#123; if(j==0 || typed.charAt(j)!= typed.charAt(j-1)) return false; &#125; &#125; return i==name.length(); &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[268. Missing Number]]></title>
    <url>%2F2018%2F11%2F02%2F268-Missing-Number%2F</url>
    <content type="text"><![CDATA[题目缺失数字 给定一个包含 0, 1, 2, ..., n 中 n 个数的序列，找出 0 .. n 中没有出现在序列中的那个数。 示例 1: 12输入: [3,0,1]输出: 2 示例 2: 12输入: [9,6,4,2,3,5,7,0,1]输出: 8 说明:你的算法应具有线性时间复杂度。你能否仅使用额外常数空间来实现? 思路认真审题；如果使用先排序在二分查找的策略，时间复杂度上明显会大于O(n)。在这里将其看作一道数学题解答就好了。根据序列长度n，可以求出不缺失数字时的和。数字缺失之后的和通过遍历一遍数组得到。最后二者相减就是要找的缺失数字。 代码12345678910class Solution &#123; public int missingNumber(int[] nums) &#123; int n = nums.length; int sum = 0; for(int i=0; i&lt;n; i++)&#123; sum += nums[i]; &#125; return n*(n+1)/2 - sum; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[排序后的数组删除重复（次数大于2）数字]]></title>
    <url>%2F2018%2F10%2F31%2F%E6%8E%92%E5%BA%8F%E5%90%8E%E7%9A%84%E6%95%B0%E7%BB%84%E5%88%A0%E9%99%A4%E9%87%8D%E5%A4%8D%EF%BC%88%E6%AC%A1%E6%95%B0%E5%A4%A7%E4%BA%8E2%EF%BC%89%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[题目给定升序排序的数组，如果数组有2个以上相同的数字，去掉他们，直到剩下2个为止。 例如： 数组A[] = [1,1,1,2,2,3] 你的算法需要返回新数组的长度5, 此时A为[1,1,2,2,3] 格式：第一行输入一个数字n，第二行输入A[n], 最后输出新数组的长度。 样例输入 1261 1 1 1 3 3 样例输出 14 思路从第3个数开始遍历到最后 ​ 如果当前位置的数字和其前两个数字都相等，则count减1 代码123456789101112131415161718192021222324package jisuanke;import java.util.Scanner;public class RmDuplicateNum &#123; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); int n = sc.nextInt(); // 输入数组长度 int[] A = new int[n]; // 通过for循环输入数组内容 for(int i=0; i&lt;n;i++) &#123; A[i] = sc.nextInt(); &#125; int count = n; for(int i=2; i&lt;n; i++) &#123; if(A[i]==A[i-1] &amp;&amp; A[i]==A[i-2]) &#123; count--; &#125; &#125; System.out.println(count); &#125;&#125;]]></content>
      <categories>
        <category>计蒜客</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[进制转换]]></title>
    <url>%2F2018%2F10%2F31%2F%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[题目实现10进制数到任意进制(2~9)的转换。 思路辗转相除法，除n倒取余。 代码1234567891011121314151617181920212223package jisuanke;import java.util.Scanner;public class DecimalConversion &#123; public static void main(String[] args) &#123; Scanner input = new Scanner(System.in); int d = input.nextInt(); // 输入需要转换的十进制数 int k = input.nextInt(); // 输入进行多少进制的转换（2-9） trans(d,k); &#125; public static void trans(int d, int k) &#123; int a[] = new int[1024]; int i; for(i=0;d!=0;i++) &#123; a[i] = d % k; d = d/k; &#125; for(i--;i&gt;=0;i--) &#123; System.out.print(a[i]); &#125; &#125;&#125;]]></content>
      <categories>
        <category>计蒜客</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客迁移到一台新电脑]]></title>
    <url>%2F2018%2F10%2F29%2FHexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%8F%A6%E4%B8%80%E5%8F%B0%E7%94%B5%E8%84%91%2F</url>
    <content type="text"><![CDATA[前言由于10月份换了一台Macbook Pro，导致自己搭建的Hexo博客一直停滞了。我想人做事一定要善始善终，不要忘记当初搭建Hexo博客的初心。于是乎，就有了这一篇文章。 迁移思路在已经推送到Github上的Hexo静态网页创建一个分支，利用这个分支来管理自己的Hexo环境文件。 迁移步骤1.在旧机器上克隆Github上面生成的静态文件到本地1git clone https://github.com/username/username.github.io.git 2.针对克隆到本地的文件中，将除去.git文件的所有文件都删除3.将旧机器中所有文件(.gitignore文件中包含的文件除外）拷贝到我们克隆下来的文件内4. 创建并切换到一个叫hexo的分支1git checkout -b hexo 5. 提交复制过来的文件到暂存取1git add -A 6.提交1git commit -m "ceate a new branch file" 7.推送到Github1git push --set-upstream origin hexo 这个时候hexo分支已经创建完毕，接下来，我们在新电脑上搭建环境。 8.新电脑配置环境安装node.js，根据自己电脑系统自行百度安装。 安装git，git相关教程推荐廖雪峰老师的git教程。 安装hexo： npm install -g hexo-cli 9.clone远程仓库到本地1git clone -b hexo https://github.com/username/username.github.io.git 10.根据package.json安装依赖1npm install *** --save 将***替换为package.json文件内的依赖包 11.开始写文章我们现在可以通过hexo n &quot;文章标题&quot; 创建一篇文章了！ 12. 提交hexo环境文件git add . git commit -m &quot;some description&quot; git push origin hexo 13.发布文章1hexo g -d 到这里，我们的Hexo博客就迁移完毕了！！以后再写文章时，只需要重复步骤11、12、13就ok啦！！]]></content>
      <categories>
        <category>“Hexo”</category>
      </categories>
      <tags>
        <tag>Hexo迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Scrapy框架的CrawlSpider类爬取当当全网商品信息]]></title>
    <url>%2F2018%2F09%2F01%2F%E5%9F%BA%E4%BA%8EScrapy%E6%A1%86%E6%9E%B6%E7%9A%84CrawlSpider%E7%B1%BB%E7%88%AC%E5%8F%96%E5%BD%93%E5%BD%93%E5%85%A8%E7%BD%91%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言本项目通过使用Scrapy框架的CrawlSpider类，对当当全网商品信息进行爬取并将信息保存至mysql数据库，当当网反爬措施是对IP访问频率的限制，所以本项目使用了中间件scrapy-rotating-proxies来管控IP代理池，有关代理ip的爬取请见我的另一篇博文。 CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类通过定义一些规则(rule)来跟进所爬取网页中的link，从爬取的网页中获取link并继续爬取。 Github地址: https://github.com/RunningGump/crawl_dangdang 依赖 scrapy 1.5.0 python3.6 mysql 5.7 pymysql 库 scrapy-rotating-proxies 库 fake-useragent 库 创建项目首先，我们需要创建一个Scrapy项目，在shell中使用scrapy startproject命令： 1234567$ scrapy startproject DangdangNew Scrapy project 'Dangdang', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in: /home/geng/DangdangYou can start your first spider with: cd Dangdang scrapy genspider example example.com 创建好一个名为Dangdang的项目后，接下来，你进入新建的项目目录： 1$ cd Dangdang 然后,使用scrapy genspider -t &lt;template&gt; &lt;name&gt; &lt;domain&gt;创建一个spider： 123$ scrapy genspider -t crawl dd dangdang.comCreated spider 'dd' using template 'crawl' in module: Dangdang.spiders.dd 此时，你通过cd ..返回上级目录，使用tree命令查看项目目录下的文件，显示如下： 1234567891011121314151617181920$ cd ..$ tree Dangdang/Dangdang/├── Dangdang│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── __pycache__│ │ ├── __init__.cpython-36.pyc│ │ └── settings.cpython-36.pyc│ ├── settings.py│ └── spiders│ ├── dd.py│ ├── __init__.py│ └── __pycache__│ └── __init__.cpython-36.pyc└── scrapy.cfg4 directories, 11 files 到此为止，我们的项目就创建成功了。 rules在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作设置了爬取规则。 参数介绍： link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。 callback： 回调函数，对link_extractor获得的链接进行处理与解析。 注意事项：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。 follow：是一个布尔(boolean)值，指定了根据规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤链接。 process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request) LinkExtrator参数介绍： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)匹配的URL不提取。 allow_domains：会被提取的链接的域名。 deny_domains：不会被提取链接的域名。 restrict_xpaths：使用Xpath表达式与allow共同作用提取出同时符合对应Xpath表达式和正则表达式的链接； 项目代码编写item.py文件123456789101112import scrapyclass DangdangItem(scrapy.Item): category = scrapy.Field() # 商品类别 title = scrapy.Field() # 商品名称 link = scrapy.Field() # 商品链接 price = scrapy.Field() # 商品价格 comment = scrapy.Field() # 商品评论数 rate = scrapy.Field() # 商品的好评率 source = scrapy.Field() # 商品的来源地 detail = scrapy.Field() # 商品详情 img_link = scrapy.Field() # 商品图片链接 编写pipeline.py文件需提前创建好数据库，本项目创建的数据库名字为dd。 123456789101112131415161718192021222324252627282930313233343536373839import pymysqlfrom scrapy.conf import settings## pipeline默认是不开启的，需在settings.py中开启class DangdangPipeline(object): def process_item(self, item, spider): ##建立数据库连接 conn = pymysql.connect(host="localhost",user="root",passwd='yourpasswd',db="dd",use_unicode=True, charset="utf8") cur = conn.cursor() # 用来获得python执行Mysql命令的方法,也就是我们所说的操作游标 print("mysql connect success") # 测试语句，这在程序执行时非常有效的理解程序是否执行到这一步 try: category = item["category"] title = item["title"] if len(title)&gt;40: title = title[0:40] + '...' link = item["link"] img_link = item['img_link'] price = item["price"] comment = item["comment"] rate = item["rate"] source = item["source"] detail = item["detail"] sql = "INSERT INTO goods(category,title,price,comment,rate,source,detail,link,img_link) VALUES ('%s','%s','%s','%s','%s','%s','%s','%s','%s')" % (category,title,price,comment,rate,source,detail,link,img_link) print(sql) except Exception as err: pass try: cur.execute(sql) # 真正执行MySQL语句，即查询TABLE_PARAMS表的数据 print("insert success") # 测试语句 except Exception as err: print(err) conn.rollback() #事务回滚,为了保证数据的有效性将数据恢复到本次操作之前的状态.有时候会存在一个事务包含多个操作，而多个操作又都有顺序，顺序执行操作时，有一个执行失败，则之前操作成功的也会回滚，即未操作的状态 else: conn.commit() #当没有发生异常时，提交事务，避免出现一些不必要的错误 conn.close() #关闭连接 return item #框架要求返回一个item对象 编写middlewares.py本项目添加了RandomUserAgentMiddleWare中间件，用来随机更换UserAgent。在middlewares.py文件的最后面添加如下中间件： 1234567891011121314151617181920from fake_useragent import UserAgentclass RandomUserAgentMiddleWare(object): """ 随机更换User-Agent """ def __init__(self,crawler): super(RandomUserAgentMiddleWare, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get("RANDOM_UA_TYPE", "random") @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua_type(): return getattr(self.ua, self.ua_type) # 取对象 ua 的 ua_type 的这个属性, 相当于 self.ua.self.ua_type request.headers.setdefault('User-Agent', get_ua_type()) 修改settings.py 文件12345678910111213141516171819202122232425262728293031BOT_NAME = 'Dangdang'SPIDER_MODULES = ['Dangdang.spiders']NEWSPIDER_MODULE = 'Dangdang.spiders'# 不遵循robots协议ROBOTSTXT_OBEY = False# 下载延迟设置为0，提高爬取速度DOWNLOAD_DELAY = 0#禁用Cookie(默认情况下启用)COOKIES_ENABLED = False # 启用所需要的下载中间件DOWNLOADER_MIDDLEWARES = &#123; 'rotating_proxies.middlewares.RotatingProxyMiddleware': 610, 'rotating_proxies.middlewares.BanDetectionMiddleware': 620, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'Dangdang.middlewares.RandomUserAgentMiddleWare': 400,&#125;# 代理IP文件路径,此处需改为你自己的路径ROTATING_PROXY_LIST_PATH = '/home/geng/Projects/Dangdang/proxy.txt'# 随机更换UserAgent RANDOM_UA_TYPE = "random"# 开启pipelineITEM_PIPELINES = &#123; 'Dangdang.pipelines.DangdangPipeline': 300,&#125; spider文件(dd.py)编写12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom Dangdang.items import DangdangItemimport reimport urllib.requestimport jsonclass DdSpider(CrawlSpider): name = 'dd' allowed_domains = ['dangdang.com'] start_urls = ['http://category.dangdang.com/'] # 分析网页链接，编写rules规则,提取商品详情页的链接 rules = ( Rule(LinkExtractor(allow=r'/cp\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html$|/pg\d+-cp\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html$', deny=r'/cp98.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html'), follow=True), Rule(LinkExtractor(allow=r'/cid\d+.html$|/pg\d+-cid\d+.html$', deny=r'/cp98.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html'), follow=True), Rule(LinkExtractor(allow=r'product.dangdang.com/\d+.html$', restrict_xpaths=("//p[@class='name']/a")), callback='parse_item', follow=False), # allow与restrict_xpath配合使用,效果很好,可以更精准筛选链接. ) # 解析商品详情页面 def parse_item(self, response): item = DangdangItem() # 实例化item item["category"] = response.xpath('//*[@id="breadcrumb"]/a[1]/b/text()').extract_first()+'&gt;'+response.xpath('//*[@id="breadcrumb"]/a[2]/text()').extract_first()+'&gt;'+response.xpath('//*[@id="breadcrumb"]/a[3]/text()').extract_first() item["title"] = response.xpath("//*[@id='product_info']/div[1]/h1/@title").extract_first() item["detail"] = json.dumps(response.xpath("//*[@id='detail_describe']/ul//li/text()").extract(),ensure_ascii=False) item["link"] = response.url item["img_link"] =json.dumps(response.xpath("//div[@class='img_list']/ul//li/a/@data-imghref").extract()) try: item["price"] = response.xpath("//*[@id='dd-price']/text()").extract()[1].strip() except IndexError as e: item["price"] = response.xpath("//*[@id='dd-price']/text()").extract()[0].strip() item["comment"] = response.xpath("//*[@id='comm_num_down']/text()").extract()[0] try: item["source"] = response.xpath("//*[@id='shop-geo-name']/text()").extract()[0].replace('\xa0至','') except IndexError as e: item["source"] = '当当自营' # 通过抓包分析,提取商品的好评率 goodsid = re.compile('\/(\d+).html').findall(response.url)[0] # 提取url中的商品id # 提取详情页源码中的categoryPath script = response.xpath("/html/body/script[1]/text()").extract()[0] categoryPath = re.compile(r'.*categoryPath":"(.*?)","describeMap').findall(script)[0] # 构造包含好评率包的链接 rate_url = "http://product.dangdang.com/index.php?r=comment%2Flist&amp;productId="+str(goodsid)+"&amp;categoryPath="+str(categoryPath)+"&amp;mainProductId="+str(goodsid) rate_date = urllib.request.urlopen(rate_url).read().decode("utf-8") # 读取包含好评率的包的内容 item["rate"] = re.compile(r'"goodRate":"(.*?)"').findall(rate_date)[0]+'%' # 用正则表达式提取好评率 yield item 使用方法在命令行中执行以下命令开始爬取： 1$ scrapy crawl dd 结果展示]]></content>
      <categories>
        <category>python3网络爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于scrapy框架爬取西刺代理并验证有效性]]></title>
    <url>%2F2018%2F08%2F02%2F%E5%9F%BA%E4%BA%8Escrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E8%A5%BF%E5%88%BA%E4%BB%A3%E7%90%86%E5%B9%B6%E9%AA%8C%E8%AF%81%E6%9C%89%E6%95%88%E6%80%A7%2F</url>
    <content type="text"><![CDATA[前言在爬取网站数据的时候，一些网站会对用户的访问频率进行限制，如果爬取过快会被封ip，而使用代理可防止被封禁。本项目使用scrapy框架对西刺网站进行爬取，并验证爬取代理的有效性，最终将有效的代理输出并存储到json文件中。 Github地址: https://github.com/RunningGump/crawl_xiciproxy 依赖 python3.6 Scrapy 1.5.0 创建项目首先，我们需要创建一个Scrapy项目，在shell中使用scrapy startproject命令： 1234567$ scrapy startproject xiciproxyNew Scrapy project 'xiciproxy', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in: /home/geng/xiciproxyYou can start your first spider with: cd xiciproxy scrapy genspider example example.com 创建好一个名为xiciproject的项目后，接下来，你进入新建的项目目录： 1$ cd xiciproxy 然后,使用scrapy genspider &lt;name&gt; &lt;domain&gt;创建一个spider： 123$ scrapy genspider xici xicidaili.comCreated spider 'xici' using template 'basic' in module: xiciproxy.spiders.xici 此时，你通过cd ..返回上级目录，使用tree命令查看项目目录下的文件，显示如下： 1234567891011121314151617181920$ cd ..$ tree xiciproxyxiciproxy├── scrapy.cfg└── xiciproxy ├── __init__.py ├── items.py ├── middlewares.py ├── pipelines.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── settings.cpython-36.pyc ├── settings.py └── spiders ├── __init__.py ├── __pycache__ │ └── __init__.cpython-36.pyc └── xici.py4 directories, 11 files 到此为止，我们的项目就创建成功了。 分析页面编写爬虫程序之间，首先需要对待爬取的页面进行分析，主流的浏览器中都带有分析页面的工具或插件，这里我们选用Chrome浏览器的开发者工具分析页面。 链接信息在Chrome浏览器中打开页面http://www.xicidaili.com/, 通过点击国内高匿代理和国内普通代理以及进行翻页操作，会发现以下规律： http://www.xicidaili.com/参数1/参数2 参数1中nn代表高匿代理，nt代表普通代理；参数2中1,2,3,4…代表页数。 数据信息爬取网页信息时一般使用高匿代理，高匿代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真是IP是隐藏的，不会认为我们使用了代理。 本部分以爬取高匿代理为例子来分析如何爬取网页的数据信息。在Chrome浏览器中打开页面http://www.xicidaili.com/nn, 并按F12键来打开开发者工具，点击Elements（元素）来查看其HTML代码，会发现每一条代理的信息都包裹在一个tr标签下，如下图所示： 再来单独对一个tr标签进行分析： 123456789101112131415161718192021222324252627&lt;tr class="odd"&gt; &lt;td class="country"&gt;&lt;img src="http://fs.xicidaili.com/images/flag/cn.png" alt="Cn"&gt;&lt;/td&gt; &lt;td&gt;115.198.35.213&lt;/td&gt; &lt;td&gt;6666&lt;/td&gt; &lt;td&gt; &lt;a href="/2018-07-20/zhejiang"&gt;浙江杭州&lt;/a&gt; &lt;/td&gt; &lt;td class="country"&gt;高匿&lt;/td&gt; &lt;td&gt;HTTPS&lt;/td&gt; &lt;td class="country"&gt; &lt;div title="0.144秒" class="bar"&gt; &lt;div class="bar_inner fast" style="width:85%"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td class="country"&gt; &lt;div title="0.028秒" class="bar"&gt; &lt;div class="bar_inner fast" style="width:96%"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;15天&lt;/td&gt; &lt;td&gt;18-08-04 15:33&lt;/td&gt; &lt;/tr&gt; 会发现：IP地址包裹在td[2]标签下，端口port包裹在td[3]标签下，类型（http/https）包裹在td[6]标签下。 程序编写分析完页面后，接下来编写爬虫。本项目主要是对xici.py进行编写，对settings.py仅做了轻微改动。 实现spider即编写xici.py文件，程序如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import scrapyimport json'''scrapy crawl xici -o out.json -a num_pages=50 -a typ=nn其中`out.json`是输出有效代理的json文件，`num_pages`是爬取页数，`typ`表示代理类型，`nn`是高匿代理，`nt`是普通代理。'''class XiCiSpider(scrapy.Spider): # 每一个爬虫的唯一标识 name = 'xici' # 使用-a选项,可以将命令行参数传递给spider的__init__方法 def __init__(self, num_pages=5, typ='nn', *args, **kwargs): num_pages = int(num_pages) self.num_pages = num_pages self.typ = typ # 定义起始爬取点 def start_requests(self): for page in range(1, self.num_pages + 1): url = 'http://www.xicidaili.com/&#123;&#125;/&#123;&#125;'.format(self.typ, page) yield scrapy.Request(url=url) # 解析response返回的网页 def parse(self, response): proxy_list = response.xpath('//table[@id = "ip_list"]/tr[position()&gt;1]') for tr in proxy_list: # 提取代理的 ip, port, scheme(http or https) ip = tr.xpath('td[2]/text()').extract_first() port = tr.xpath('td[3]/text()').extract_first() scheme = tr.xpath('td[6]/text()').extract_first() # 使用爬取到的代理再次发送请求到http(s)://httpbin.org/ip, 验证代理是否可用 url = '%s://httpbin.org/ip' % scheme proxy = '%s://%s:%s' % (scheme, ip, port) meta = &#123; 'proxy': proxy, 'dont_retry': True, 'download_timeout': 5, # 下面的ip字段是传递给check_available方法的信息,方便检测是否可隐藏ip '_proxy_ip':ip, &#125; yield scrapy.Request(url, callback=self.check_available, meta=meta, dont_filter=True) def check_available(self, response): proxy_ip = response.meta['_proxy_ip'] # 判断代理是否具有隐藏IP功能 if proxy_ip == json.loads(response.text)['origin']: yield&#123; 'proxy': response.meta['proxy'] &#125; 修改配置文件 更改USER_AGENT：西刺代理网站会通过识别请求中的user-agent来判断这次请求是真实用户所为还是机器所为。 不遵守robots协议：网站会通过robots协议告诉搜索引擎那些页面可以抓取，哪些不可以抓取，而robots协议大多不允许抓取有价值的信息，所以咱们不遵守。 禁用cookies：如果用不到cookies，就不要让服务器知道你的cookies。 文件settings.py中的改动如下： 123USER_AGENT = &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36&quot;ROBOTSTXT_OBEY = TrueCOOKIES_ENABLED = False 在编写好xici.py和settings.py后，我们的项目就大功告成啦！ 使用方法使用方法就是在命令行中执行以下命令即可： 1$ scrapy crawl xici -o out.json -a num_pages=10 -a typ=nn 其中out.json是最终输出有效代理的json文件，num_pages是爬取页数，typ表示要爬取的代理类型，nn是高匿代理，nt是普通代理。 提示 ：程序在验证代理有效性的过程中，对于无效的代理会抛出超时异常，不要管这些异常，让程序继续执行直到结束。]]></content>
      <categories>
        <category>python3网络爬虫</category>
      </categories>
      <tags>
        <tag>西刺代理</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
