<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[成功解决在hexo中无法显示数学公式的问题]]></title>
    <url>%2F2018%2F12%2F05%2F%E6%88%90%E5%8A%9F%E8%A7%A3%E5%86%B3%E5%9C%A8hexo%E4%B8%AD%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言基于Hexo搭建的个人博客，在默认情况下渲染数学公式的时候是会出现问题的。下面的截图是我的先前的博客中出现的公式渲染错误： 经过了一波百度操作后，成功将问题解决，下面是解决后的截图： 下面我将我的操作写在下面，供需要的人参考。 解决步骤1更换Hexo的markdown渲染引擎先后执行下面的两条命令，第一条表示将默认的渲染引擎hexo-renderer-marked卸载，第二条命令是安装hexo-renderer-kramed渲染引擎，此渲染引擎修改了hexo-renderer-marked渲染引擎的一些bug。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 2修改node_modules\kramed\lib\rules\inline.js文件hexo-renderer-marked渲染引擎仍然存在一些语义冲突问题，到博客的根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改： 12// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 这一步是在原基础上取消了对\,{,}的转义(escape)。 同时把第20行的em变量也要做相应的修改。 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 3在主题中开启mathjax开关到博客根目录下，找到themes/next/_config.yml，把math默认的flase修改为true，具体如下： 12345# Math Equations Render Supportmath: enable: true per_page: true engine: mathjax 4在文章的Front-matter里打开mathjax开关如果你写的文章里面用到了数学公式，需要在文章Front-matter里打开mathjax开关。如果用不到数学公式，则不需要管它。 123456---title: index.htmldate: 2018-12-5 01:30:30tags:mathjax: true-- 5重启hexo12hexo clean #清除缓存文件hexo g -d #生成并部署hexo 到这里，hexo中无法显示数学公式的问题就得到解决了！ 参考文献在Hexo中渲染MathJax数学公式]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>mathjax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[破解含语序问题的点击验证码]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%A0%B4%E8%A7%A3%E5%90%AB%E8%AF%AD%E5%BA%8F%E9%97%AE%E9%A2%98%E7%9A%84%E7%82%B9%E5%87%BB%E9%AA%8C%E8%AF%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[设计思路前言国家企业信用信息公示系统中的验证码是按语序点击汉字，如下图所示： 即，如果依次点击：‘无’，‘意’，‘中’，‘发’，‘现’，就会通过验证。 本项目的破解思路主要分为以下步骤： 使用目标探测网络YOLOV2进行汉字定位 设计算法进行汉字切割 使用darknet的分类器进行汉字识别 设计算法进行汉字纠错与语序识别 Github仓库直通车 汉字定位与汉字识别本项目的汉字定位和汉字识别部分都是基于darknet框架进行训练的。本项目对它们使用的训练网络并没有太高要求，只需懂得如何使用darknet就可以了，关于如何使用darknet框架训练汉字定位模型和汉字识别模型可查阅模型训练文档以及官方文档的YOLO和Train a Classifier部分。那么，下面主要对汉字切割和语序识别进行讲解，最后再对整个破解程序进行讲解。 汉字切割算法123456789101112131415161718192021222324252627282930313233343536373839def seg_one_img(img_path, rets): img = cv2.imread(img_path) hanzi_list = [] # 用于记录每个汉字对应的坐标：key为切割后汉字图片路径，value为中心点坐标 # 对定位框进行遍历 for ret in rets: per_dict = &#123;&#125; if ret[1] &gt; 0.5: # 只取置信度大于0.5的定位框 coordinate = ret[2] # ret[2]为定位器返回的归一化坐标（x,y,w,h） center = (int(coordinate[0]*344), int(coordinate[1]*384)) #汉字定位框中心点坐标 origin = (coordinate[0] - coordinate[2]/2, coordinate[1] - coordinate[3]/2) # 汉字定位框左上角坐标（归一化） # 将定位框向四周均匀扩大2个像素，尽量将整个汉字切割下来。 x = int(origin[0]*344 - 2) x_plus_w =int((origin[0] + coordinate[2])*344 + 4) y = int(origin[1]*384 - 2) y_plus_h = int((origin[1] + coordinate[3])*384 + 4) # 扩大后的定位框可能会出现越界的可能，如一个紧挨着图片边缘的汉字，fix函数调整越界的定位框 x, y, x_plus_w, y_plus_h = fix(x,y,x_plus_w,y_plus_h) # 下面对图片进行切割，并保存 try: hanzi_img = img[y:y_plus_h, x:x_plus_w] # 切割 normal_img = cv2.resize(hanzi_img, (65,65), interpolation=cv2.INTER_CUBIC) # 将截取的图片规范化为65*65*3 path = 'hanzi_img/&#123;&#125;_label.jpg.format(timestamp()) cv2.imwrite(path, normal_img) per_dict[path] = center hanzi_list.append(per_dict) except: print('#'*20) print('存在不规则的图片') return hanzi_list# 修正定位框的坐标，如果扩大后的定位框越界则将其设置为边界坐标def fix(x, y, x_plus_w, y_plus_h ): x = 0 if x &lt; 0 else x y = 0 if y &lt; 0 else y x_plus_w = 384 if x_plus_w &gt; 384 else x_plus_w y_plus_h = 344 if y_plus_h &gt; 344 else y_plus_h return x, y, x_plus_w, y_plus_h seg_one_img函数是对一张验证码图片进行汉字切割，切割后的汉字图片保存在当前路径下的hanzi_img文件夹中，并且返回由字典（key为汉字图片路径，value为坐标）组成的列表。需要注意的是，定位接口返回的定位框信息均是归一化信息，需要转换成实际的坐标信息，验证码图片大小信息为：344 × 384 × 3。如（0.25，,75）&gt;&gt; (0.25×344,0.75×384) 算法大体思路： 切割一张图片（图片路径，定位接口返回的定位框信息）： 遍历定位框信息，对置信度大于0.5的定位框进行如下操作： 计算汉字定位框中心坐标和左上角坐标； 将汉字定位框向四周均匀扩大两个像素； 对越界的坐标进行修正； 对汉字进行切割； 定位框向四周扩大两个像素的目的：尽量将整个汉字切割下来。因为经过测试，有些定位框定位正确但是IOU不是很高，即汉字的某一小部分可能在定位框外部。扩大定位框可以更好的用于后面的汉字识别。 语序识别算法语序识别算法结合了使用结巴分词识别语序和使用搜索引擎识别语序两个函数，下面分别对两个函数进行讲解。 使用结巴分词识别语序本部分使用的是 Python 中文分词词库jieba，关于结巴分词的基础知识请先阅读结巴分词Github文档，下面对使用结巴分词识别语序进行讲解。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 结巴分词 识别语序def recog_order_jieba(str): l = len(str) # l表示输入字符串个数 word_list = _permutation(str) # 获得该字符串的所有排列方式 possible_words = [] # 用来存放语序可能正确的词 for word in word_list: # 编列所有排列方式 seg_list = jieba.lcut(word, cut_all=True ) # 对某一种排列方式使用结巴分词 index = find_longest(seg_list) # 寻找结巴分词返回的列表中字符串最长的索引，并返回 if len(seg_list[index]) == l: # 若最长的字符串与输入的字符串长度相同，则加入可能正确列表 possible_words.append(seg_list[index]) if len(possible_words) ==1: # 遍历完后，若可能正确的列表只有一个元素，那么他就是正确的，返回 return possible_words[0] elif len(possible_words) &gt;1: # 若有可能正确列表中若有多个元素，则选取词频高的返回 return highest_frequency(possible_words) else: # 如果可能正确的列表元素为0，则返回0 return 0 # 获得汉字的所有排列方式def _permutation(str, r = None): word_list = list(permutations(str, r)) for i in range(len(word_list)): word_list[i] = ''.join(word_list[i]) return word_list# 寻找列表中最长的词def find_longest(list): l = 0 index = 0 for i,word in enumerate(list): if len(word) &gt; l: l = len(word) index = i return index# 输入词列表，返回结巴分词内词频最高的词def highest_frequency(possible_words): word_dict = file2dict('dict.txt') possible_dict = &#123;&#125; for possible_word in possible_words: possible_dict[word_dict[possible_word]] = possible_word sorted = sortedDictValues(possible_dict) print(sortedList) return sortedList[-1][1]# 对输入的字典根据key大小排序def sortedDictValues(di): return [(k,di[k]) for k in sorted(di.keys())]# 将文件数据转换为字典def file2dict(filename): with open(filename) as f: array_lines = f.readlines() returnDict = &#123;&#125; # 以下三行解析文件数据到列表 for line in array_lines: line = line.strip() listFromLine = line.split() returnDict[listFromLine[0]] = int(listFromLine[1]) return returnDict 下面我通过一个具体的实例来讲解算法思路: 输入：‘到马功成’ 获得字符串长度： l =4 获得字符串的全排列 1['到马功成', '到马成功', '到功马成', '到功成马', '到成马功', '到成功马', '马到功成', '马到成功', '马功到成', '马功成到', '马成到功', '马成功到', '功到马成', '功到成马', '功马到成', '功马成到', '功成到马', '功成马到', '成到马功', '成到功马', '成马到功', '成马功到', '成功到马', '成功马到'] 对每一个排列进行结巴分词，并打印其中字符串最长元素的索引 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748['到', '马', '功', '成']0['到', '马', '成功']2['到', '功', '马', '成']0['到', '功', '成', '马']0['到', '成', '马', '功']0['到', '成功', '马']1['马到功成']0['马到成功', '成功']0['马', '功', '到', '成']0['马', '功', '成', '到']0['马', '成', '到', '功']0['马', '成功', '到']1['功', '到', '马', '成']0['功', '到', '成', '马']0['功', '马', '到', '成']0['功', '马', '成', '到']0['功', '成', '到', '马']0['功', '成', '马', '到']0['成', '到', '马', '功']0['成', '到', '功', '马']0['成', '马', '到', '功']0['成', '马', '功', '到']0['成功', '到', '马']0['成功', '马', '到']0 遍历完之后，将l=4的字符串加入possible_words列表 1[&apos;马到功成&apos;, &apos;马到成功&apos;] # possible_words列表 现在有两个词语语序是可能正确的，由于结巴分词词库中的词语是有词频的，比如： 12345678............马利诺夫斯基 3 nrt马到功成 3 i马到成功 313 i马刺进 2 nr............ 每行的第二个元素代表词频，所以我们可以通过比较词频来确定最终的语序正确的词： 1马到成功 使用搜索引擎识别语序123456789101112131415161718192021222324252627282930313233343536373839404142# 搜索引擎搜索关键字,返回相关列表def search_engine(word): headers = &#123; 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36' &#125; r = requests.get('https://www.baidu.com/s?wd=' + word, headers=headers) html = etree.HTML(r.text) related_words1 = html.xpath('//*[@id="rs"]/table//tr//th/a/text()') related_words2 = html.xpath('//div[@id="content_left"]//a//em/text()') related_words = related_words1 + related_words2 return related_words # 调用一次线程，每一个线程对输入字符串进行百度搜索，返回相关词的列表def search(word): related_words = search_engine(word) global all_related all_related = all_related + related_words# 通过搜索引擎识别语序def search_engine_recog(str): word_list = _permutation(str) # 获得排列 global flags flags = [0] * len(word_list) # 标志位 threads = [] for word in word_list: # 遍历所有可能的排列组合，进行百度搜索 thread = threading.Thread(target=search, args=[word]) threads.append(thread) thread.start() for thread in threads: thread.join() global all_related # 记录所有排列组合进行百度搜索后返回的列表 for i,word in enumerate(word_list): # 遍历所有排列 flag = 0 for related_word in all_related: # 对每一个排列统计在所有相关词语列表中出现的次数 if word in related_word: flag = flag + 1 flags[i] = flag all_related = [] # 清空 index = flags.index(max(flags)) # 找到标志位最大的索引 return word_list[index] 同样，这里仍然用一个实例来讲解该算法思路： 输入：’现无中意发’ 获得输入字符串的排列： 1['现无中意发', '现无中发意', '现无意中发', '现无意发中', '现无发中意', '现无发意中', '现中无意发', '现中无发意', '现中意无发', '现中意发无', '现中发无意', '现中发意无', '现意无中发', '现意无发中', '现意中无发', '现意中发无', '现意发无中', '现意发中无', '现发无中意', '现发无意中', '现发中无意', '现发中意无', '现发意无中', '现发意中无', '无现中意发', '无现中发意', '无现意中发', '无现意发中', '无现发中意', '无现发意中', '无中现意发', '无中现发意', '无中意现发', '无中意发现', '无中发现意', '无中发意现', '无意现中发', '无意现发中', '无意中现发', '无意中发现', '无意发现中', '无意发中现', '无发现中意', '无发现意中', '无发中现意', '无发中意现', '无发意现中', '无发意中现', '中现无意发', '中现无发意', '中现意无发', '中现意发无', '中现发无意', '中现发意无', '中无现意发', '中无现发意', '中无意现发', '中无意发现', '中无发现意', '中无发意现', '中意现无发', '中意现发无', '中意无现发', '中意无发现', '中意发现无', '中意发无现', '中发现无意', '中发现意无', '中发无现意', '中发无意现', '中发意现无', '中发意无现', '意现无中发', '意现无发中', '意现中无发', '意现中发无', '意现发无中', '意现发中无', '意无现中发', '意无现发中', '意无中现发', '意无中发现', '意无发现中', '意无发中现', '意中现无发', '意中现发无', '意中无现发', '意中无发现', '意中发现无', '意中发无现', '意发现无中', '意发现中无', '意发无现中', '意发无中现', '意发中现无', '意发中无现', '发现无中意', '发现无意中', '发现中无意', '发现中意无', '发现意无中', '发现意中无', '发无现中意', '发无现意中', '发无中现意', '发无中意现', '发无意现中', '发无意中现', '发中现无意', '发中现意无', '发中无现意', '发中无意现', '发中意现无', '发中意无现', '发意现无中', '发意现中无', '发意无现中', '发意无中现', '发意中现无', '发意中无现'] 对每一个排列进行百度搜索返回相关词。 其中的百度搜索是通过爬虫实现的，爬取的结点主要有两部分：1.每次搜索结果词条中红色的词。2.每次搜索结果最下面的相关搜索中的词。 12345['中意隆鑫航发基地', '我对你中意红包怎么发', '中意保险几号发工资', '当时不知曲中意现已成为曲中人', '中意空调现E5怎么办', '初闻不知曲中意现已成为曲中人', '中意', '中意在线', '初闻不知曲中意,再听已是曲中人', '发中意', '没有中意', '中意', '中意', '中意', '中意', '中意', '中意','中意', '中意', '没有', '中意', '中意', '没有', '中意', '中意隆鑫航发基地', '我对你中意红包怎么发', '中意保险几号发工资', '当时不知曲中意现已成为曲中人', '中意空调现E5怎么办', '初闻不知曲中意现已成为曲中人', '中意', '中意在线', '初闻不知曲中意,再听已是曲中人', '现发中意无','没有中意', '中意', '没有', '中意', '没有', '中意', '中意', '没有', '中意', '中意', '没有', '中意', '没有', '中意', '中发发型', '中发', '中发卷发', '中发烫发', '中发发型图片', '中发图片', '中发编发', '中发烫发图片', '中发白', '中发无意现', '中发', '中发', '无意', '中发', '中发', '中发', '中发', '中发', '中意隆鑫航发基地', '我对你中意红包怎么发', '中意保险几号发工资', '当时不知曲中意现已成为曲中人', '中意空调现E5怎么办', '初闻不知你', '中意在线用户登录', '我只中意你', '中意保险可靠吗', '中意', '中意', '中意', '无中意', '无中意', '中意', '发现', '中意', '中意', '无中意', '中意', '无中意', '意什么什么发', '意()()发', '发现的近意词是什么', '发现的进意词', '发现意', '微信里的发现是什么意是', '意料之中什么意思', '中译意', '发现', '无意中发现', ...................................................................................... ......................all_realated列表比较长，中间部分的词省略............................ ..................................................................................... '无意', '发现', '无意中发现', '无意中发现', '无意中', '无意中发现', '初闻不知曲中意现已成为曲中人', '中意', '中意在线', '初闻不知曲中意,再听已是曲中人', '发中意无', '没有中意', '中意', '无中意', '没有', '中意', '没有中意', '中意', '没有', '中意', '中意', '没有', '中意隆鑫航发基地', '我对你中意红包怎么发', '中意保险几号发工资', '当时不知曲中意现已成为曲中人', '中意空调现E5怎么办', '意无限', '意无限', '意无限', '意无限', '意无限', '意无限', '意无限', '有意瞄准无意击发', '无意击发', '有意瞄准无意击发意思', '有意激无意发', '无意和别人换了鞋有什么说发', '无意发了视频', '有意瞄准,无意击发解释', '有意瞄准无意击发柴静', '无意栽花犹发蕊', '无意', '无意', '无意', '中无意', '无意', '无意', '无意', '发发', '无意', '无意', '中无意', '无意', '意什么什么发', '意()()发', '发意生是什么意思', '发意症', '意发', '意发游戏', '意什么发成语接龙', '向发意', '意发股份', '无意中', '无意中', '无意中', '无意中', '无意中', '无意中', '无意中', '无意中', '无意中', '无意中', '无意中', '无意中', '意什么什么发', '意()()发', '发意生是什么意思', '发意症', '意发', '意发游戏', '意什么发成语接龙', '向发意', '意发股份', '无意', '无意', '无意', '无意', '没有', '无意', '无意', '无意', '发发无意', '无意', '意什么什么发', '意()()发', '发意生是什么意思', '发意症', '意发', '意发游戏', '意什么发成语接龙', '向发意', '意发股份', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '发意中现无', '意中', '意中', '意中', '意什么什么发', '意()()发', '发意生是什么意思', '发意症', '意发', '意发游戏', '意什么发成语接龙', '向发意', '意发股份', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '意中', '发意中无现', '意中', '意中'] 通过一个嵌套循环来统计每一个排列在all_relalated列表中出现的次数（排列是列表元素的子串）。 1[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 125, 0, 0, 0, 0, 1,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1] 找到标志位最大的索引，返回word_list列表中该索引值对应的排列。 1无意中发现 完整破解程序程序讲解通过多次试验会发现，使用结巴分词识别语序和搜索引擎识别语序各有利弊，使用结巴分词的优点是速度很快，缺点是对于一些不是词语的语序识别会识别不出来。而搜索引擎识别语序，语序识别能力强，但是比较慢。所以在破解程序中我将二者结合了一下，充分使用了各自的优点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# -*- coding: utf-8 -*-from darknet import load_net, load_meta, detect, classify, load_imagefrom segment import seg_one_img, load_dtc_modulefrom recog_order import search_engine_recog, recog_order_jiebaimport timeimport cv2from PIL import Imageimport numpy as npimport copyimport osfrom itertools import permutationsfrom functools import reduce# 求多个列表的组合def combination(*lists): total = reduce(lambda x, y: x * y, map(len, lists)) retList = [] for i in range(0, total): step = total tempItem = [] for l in lists: step /= len(l) tempItem.append(l[int(i/step % len(l))]) retList.append(tuple(tempItem)) return retList # 加载模块def load_clasiify_module(cfg, weights, data): net = load_net(cfg, weights, 0) meta = load_meta(data) return net, meta # 使用新字典记录坐标,注意字典是无序的！！def recordCoordinate(wordList, hanziList): center = &#123;&#125; for i in range(len(wordList)): center[wordList[i]] = [center for center in hanziList[i].values()][0] return center# 破解函数def crack(img_path, dtc_modu, classify_modu, k): # 定位汉字,得到多个矩形框 print('\n'*2 + '定位汉字' + '\n' + '*'*80) d = time.time() rets = detect(dtc_modu[0], dtc_modu[1], img_path.encode()) print('定位汉字耗时&#123;&#125;'.format(time.time() - d)) l = len(rets) # 设置阈值 if l &gt; k: return 0 # 切割图片，得到切割后的汉字图片 print('\n'*2 + '切割图片' + '\n' + '*'*80) s = time.time() hanzi_list = seg_one_img(img_path, rets) print(hanzi_list) print('切割图片耗时&#123;&#125;'.format(time.time() - s)) # 汉字识别，得到汉字字符串 print('\n'*2 + '汉字识别' + '\n' + '*'*80) r = time.time() all_hanzi_lists = [] # 存储所有汉字的列表 # 提取路径存入列表 paths = [] for per in hanzi_list: paths.extend([i for i in per.keys()]) for path in paths: # 对切割的汉字图片进行遍历 hanzis = [] img = load_image(path.encode(), 0 , 0) res = classify(classify_modu[0], classify_modu[1], img) print(res[0:5]) if res[0][1] &lt; 0.95: # 对置信度&lt;0.95的汉字 for hz in res[0:5]: # 对识别的top5进行遍历,此处可修改 hanzi = ('\\' + hz[0].decode('utf-8')).encode('utf-8').decode('unicode_escape') hanzis.append(hanzi) else: hanzi = ('\\' + res[0][0].decode('utf-8')).encode('utf-8').decode('unicode_escape') hanzis.append(hanzi) all_hanzi_lists.append(hanzis) print(all_hanzi_lists) hanzi_combination = combination(*all_hanzi_lists) hanzi_combination_connect = [] for words in hanzi_combination: hanzi_combination_connect.append(''.join(words)) print(hanzi_combination_connect) print('汉字识别耗时&#123;&#125;'.format(time.time() - r)) # 识别语序 hanzi_center = [] jieba_flag = 0 o = time.time() print('\n'*2 + '语序识别' + '\n' + '*'*80) for words in hanzi_combination_connect: # 对每一个组合进行结巴分词 # 此处对汉字的坐标进行记录 hanzi_center = recordCoordinate(words, hanzi_list) print(hanzi_center, 'jiaba') o = time.time() rec_word_possible = recog_order_jieba(words) if rec_word_possible: # 如果遇到正确的词，则标志位置1 jieba_flag = 1 break if jieba_flag: rec_word = rec_word_possible else: hanzi_center = recordCoordinate(hanzi_combination_connect[0], hanzi_list) print(hanzi_center,'engine') rec_word = search_engine_recog(hanzi_combination_connect[0]) print('语序识别结果:&#123;&#125;'.format(rec_word)) print('语序识别耗时&#123;&#125;'.format(time.time() - o)) # 按正确语序输出坐标 print('\n'*2 + '最终结果' + '\n' + '*'*80) centers = [] for i in rec_word: centers.append(hanzi_center[i]) print('正确语序的坐标：&#123;&#125;'.format(centers)) print('总耗时&#123;&#125;'.format(time.time() - d)) print(rec_word) return centers 下面依然用一个实例来讲解破解程序思路： 输入： 1.使用汉字定位模型定位汉字，得到4个定位框信息，每个定位框的第一个元素为hanzi类，第二个元素为该定位框的置信度，第三个元素为定位框的归一化坐标信息（x，y，w，h）。 1[(b'hanzi', 0.8764635920524597, (0.672152578830719, 0.355495423078537, 0.17341256141662598, 0.16976206004619598)), (b'hanzi', 0.8573136329650879, (0.625790536403656, 0.7956624627113342, 0.15850003063678741, 0.13232673704624176)), (b'hanzi', 0.857090175151825, (0.8480002284049988, 0.5595549941062927, 0.18965952098369598, 0.1373395025730133)), (b'hanzi', 0.8561009168624878, (0.29499194025993347, 0.49679434299468994, 0.16142778098583221, 0.16253654658794403))] 2.根据定位框切割图片，输出由字典组成的列表（key为汉字图片的相对路径，value为汉字的中心坐标）。 1[&#123;'hanzi_img/15343037353537.jpg': (231, 136)&#125;, &#123;'hanzi_img/15343037353541.jpg': (215, 305)&#125;, &#123;'hanzi_img/15343037353543.jpg': (291, 214)&#125;, &#123;'hanzi_img/15343037353546.jpg': (101, 190)&#125;] 3.使用汉字识别模型识别汉字，因为汉字识别会有识别错误的情况出现，为了一定程度上纠正错误，我们针对汉字识别置信度小于0.95的汉字，先选取其top5，然后对汉字识别结果的这几个字进行组合。 1234[(b'u7269', 0.9999469518661499), (b'u7545', 1.4645341252617072e-05), (b'u626c', 8.120928214339074e-06), (b'u629b', 6.87056399328867e-06), (b'u573a', 5.69164603803074e-06)][(b'u4e73', 0.8303858637809753), (b'u96c5', 0.015525326132774353), (b'u90e8', 0.015043874271214008), (b'u578b', 0.008989457972347736), (b'u64ad', 0.008476710878312588)][(b'u52a8', 0.9996626377105713), (b'u5e7c', 7.360828021774068e-05), (b'u7ead', 3.684992407215759e-05), (b'u9645', 2.4325390768353827e-05), (b'u529f', 2.07898483495228e-05)][(b'u901a', 0.5683982372283936), (b'u57d4', 0.09509918093681335), (b'u94fa', 0.0750967487692833), (b'u54fa', 0.02881033532321453),(b'u5398', 0.009910903871059418)] 可以发现第二个和第四个汉字的置信度低于0.95，对置信度低于0.95的汉字选取其top5。 1[['物'], ['乳', '雅', '部', '型', '播'], ['动'], ['通', '埔', '铺', '哺', '厘']] 可以发现，若仅选取置信度最高的，汉字识别结果是物，乳，动，通，这样就是识别错了。所以我们对置信度低的先选取其top5。 对四个汉字列表进行组合得到： 1['物乳动通', '物乳动埔', '物乳动铺', '物乳动哺', '物乳动厘', '物雅动通', '物雅动埔', '物雅动铺', '物雅动哺', '物雅动厘', '物部动通', '物部动埔', '物部动铺', '物部动哺', '物部动厘', '物型动通', '物型动埔', '物型动铺', '物型动哺', '物型动厘', '物播动通', '物播动埔', '物播动铺', '物播动哺', '物播动厘'] 4.结合结巴分词和搜索引擎识别语序。对上一步中获得的组合进行遍历，先使用结巴分词识别语序，若结巴分词能识别出来，则直接返回；若结巴分词识别不出来，则仅对置信度最高的组合使用搜索引擎识别语序，将识别结果返回。本例中，结巴分词能正确识别，返回： 1哺乳动物 5.返回汉字对应的坐标。 1[(101, 190), (215, 305), (291, 214), (231, 136)] 总结：结巴分词识别一种汉字组合语序的时间大约为：0.000××（和电脑配置有关系），所以对多个组合进行遍历耗时也不会很多。而使用搜索引擎对一种组合识别语序耗时1-15妙（根据汉字个数有所不同），所以在结巴分词识别不出来时，仅对置信度最高的组合进行搜索引擎识别语序。这样的话，整体情况会比较不错。大部分语序识别耗时在1s以内，少部分通过搜索引擎识别的则会耗时2-16秒内，根据汉字个数有多不同。 测试正确率文件准备： python/valid文件和python/valid.txt文件，其中python/valid文件内存放的是验证码图片，python/valid.txt内存放的是验证码图片文件名及其对应的正确语序，如下： 1234567............verifyCode1531875004.jpg--研究委员会verifyCode1531873597.jpg--求才若渴verifyCode1531874810.jpg--不久的将来............ 测试脚本： 1234567891011121314151617181920212223# 加载汉字定位模型print('\n'*2 + '加载模型' + '\n' + '*'*80)dtc_modu = load_dtc_module(b'../cfg/yolo-origin.cfg', b'../jiyan/backup/yolo-origin_49200.weights', b'../cfg/yolo-origin.data') # 加载汉字识别模型classify_modu = load_clasiify_module(b"../cfg/chinese_character.cfg", b"../chinese_classify/backup/chinese_character.backup", b"../cfg/chinese.data")cwd = os.getcwd()IMG_DIR = cwd.replace("python", "python/valid/")with open('valid.txt')as f: lines = f.readlines()right = 0num = len(lines)for line in lines: line = line.strip() rec_word = crack(IMG_DIR + line[:24], dtc_modu, classify_modu, 5) if rec_word == line[26:]: right = right + 1 elif rec_word == 0: num = num - 1 else: print('#'*20 + line[26:]+' ' + rec_word)print('正确率=&#123;&#125;'.format(right/num)) 注意：测试接口正确率的时候，需要将破解接口最后的返回值return centers改为return rec_word。 模型训练文档本部分主要讲解如何使用定位器和分类器，其中包括训练数据准备、模型训练以及训练结果评估。 依赖 python3.6 opencv3 numpy 文件结构该文件结构图仅列出了一些比较重要的文件，并对文件作用进行了注解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051.├──Makefile darknet配置文件├──README.md├── cfg│ ├── chinese.data 分类器训练配置文件 │ ├── chinese_character.cfg 分类器网络配置文件│ ├── yolo-origin.cfg YOLOV2定位器网络配置文件│ ├── yolo-origin.data YOLOV2定位器训练配置文件│ ├── yolov3.cfg YOLOV3定位器网络配置文件| └── yolov3.data YOLOV3定位器训练配置文件├── chinese_classify│ ├── backup 分类器权重存储文件│ ├── data │ │ ├── train 分类器训练集│ │ ├── valid 分类器验证集│ │ ├── labels.txt 分类器所有训练样本的标签│ │ ├── train.list 分类器所有训练样本的路径│ │ └── valid.list 分类器所有验证样本的路径│ ├── new_img 分类器样本标记后存储在该文件夹│ ├── old_img 分类器样本标记前存储在该文件夹│ └── label_hanzi.py 标记分类器样本的脚本├── darknet darknet二进制文件├── examples├── getmap.py 计算定位器的mAP├── include├── jiyan│ ├── backup 定位器权重存储文件 │ ├── data│ │ ├── train 定位器训练集│ │ ├── valid 定位器验证集│ │ ├── train.txt 定位器所有训练样本的路径│ │ ├── valid.txt 定位器所有验证样本的路径│ │ └── yolo.names 定位器标签仅一个，hanzi│ ├── get_pic.py 爬取gsxt网站验证码的脚本│ └── raw_img 最初爬取的2W张极验验证码图片（汉字识别已用过）├── python│ ├── hanzi_img 破解验证码时，存储的切割汉字图片的文件夹│ ├── valid 测试集-500张（测试破解接口的准确率）│ ├── valid.txt 测试集标记文件│ ├── crack_pro.py 具有一定汉字纠错能力的验证码破解接口│ ├── darknet.py 定位器和分类器调用接口│ ├── recog_order.py 识别语序接口│ ├── segment.py 切割汉字接口│ └── dict.txt 识别语序时用到的带词频的字典├── results 生成的anchor.txt会存在此文件├── scripts├── src darknet源代码└── tools ├── voc_label.py .xml标签转换为.txt标签 ├── generate_anchorsv2.py YOLOV2生成anchors脚本 └── generate_anchorsv3.py YOLOV3生成anchors脚本 Makefile文件配置1234567891011121314151617181920212223GPU=1 # 置1，使用GPU训练CUDNN=0 OPENCV=1 # 置1，开启opencvOPENMP=0DEBUG=0# 若make时，下面这行出现问题，可强制执行此行ARCH= -D_FORCE_INLINES -gencode arch=compute_30,code=sm_30 \ -gencode arch=compute_35,code=sm_35 \ -gencode arch=compute_50,code=[sm_50,compute_50] \ -gencode arch=compute_52,code=[sm_52,compute_52]# -gencode arch=compute_20,code=[sm_20,sm_21] \ This one is deprecated?# This is what I use, uncomment if you know your arch and want to specify# ARCH= -gencode arch=compute_52,code=compute_52VPATH=./src/:./examplesSLIB=libdarknet.soALIB=libdarknet.aEXEC=darknetOBJDIR=./obj/............ 提示：修改Makefile或源码后，需在命令行先敲击make clean命令，再敲击make命令，方能生效。 定位器训练训练数据准备样本的获取训练定位器样本的获取方法：爬取国家企业信用信息公示系统，使用脚本jiyan/get_pic.py进行爬取即可。 样本的标注使用标注软件labelimg对样本图片进行标注。有关labelimg软件的安装与使用请自行百度，这里不再赘述。 通过labelimg将图片标注后，会生成该图片对应的.xml标签，训练数据时需要的是.txt标签，我们需要将.xml标签转化为.txt标签。转换脚本见tools/voc_label.py。 文件准备 定位器网络配置文件cfg/yolo-origin.cfg 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[net]# Trainingbatch=64 # 每batch个样本更新一次参数subdivisions=16 # 如果内存不够大将batch分割为subdivisions个子batch，每个 # 子batch的大小为batch/subdivisionsheight=416 # input图像的高width=416 # input图像的宽channels=3 # input图像的通道数momentum=0.9 # 梯度下降法中一种加速技术，建议配置0.9decay=0.0005 # 衰减权重正则项，防止过拟合angle=0 # 通过旋转角度来生成更多训练样本saturation = 1.5 # 通过调整饱和度来生成更多训练样本exposure = 1.5 # 通过调整曝光度来生成更多训练样本hue=.1 # 通过调整色调来生成更多训练样本learning_rate=0.001 # 学习率burn_in=1000 max_batches = 80200 # 训练达到max_batches后停止学习policy=steps # 调整学习率的policysteps=40000,60000 # 根据batch_bum调整学习率scales=.1,.1 # 学习率的变化比例，累计相乘:迭代达到40000，学习率衰减十倍 # 60000迭代时，学习率又会在前一个学习率的基础上衰减十倍 [convolutional] batch_normalize=1filters=32size=3stride=1pad=1activation=leaky............[convolutional]size=1stride=1pad=1filters=30 # region前最后一个卷积层的filters数是特定的，本项目为30activation=linear[region]anchors = 2.1882101346810248, 1.7273508247089326, 2.5877106474986844, 2.3101804619114943, 1.8459417510394331, 1.7319281925870702, 2.125632169606032, 2.0649405635139693, 2.458238797504399, 1.9738465447154578 # 预选框，可以通过generate_anchorsv2.py文件生成bias_match=1classes=1 # 网络需要识别的物体种类数，本项目为1即：hanzicoords=4 # 每个box的4个坐标：tx,ty,tw,thnum=5 # 每个网格单元预测几个box，和anchors的数量一致softmax=1 # 使用softmax做激活函数jitter=.3 # 通过抖动增加噪声来抑制过拟合rescore=1object_scale=5 noobject_scale=1class_scale=1coord_scale=1absolute=1thresh=.6random=1 # random为１时，会启用Multi-Scale Training，随机使用不同 # 尺寸的图片进行训练。 重要参数讲解： filter应为30，其计算公式为： filter = num\cdot (classes + coord + 1)30 = 5\cdot (1 + 4 + 1) num代表每个网格单元预测几个box。 classes代表一共有多少个类，本项目为1。 coord代表回归的四个位置，分别为&lt;x&gt;&lt;y&gt;&lt;width&gt;&lt;height&gt;代表物体中心点相对位置以及物体相对大小。如不的标签为0 0.20 0.76 0.16 0.16 ， 忘的标签为0 0.75 0.25 0.15 0.17，初的标签为0 0.25 0.20 0.17 0.17，心的标签为0 0.60 0.62 0.16 0.18 这张图的标签为： 12340 0.25 0.20 0.17 0.170 0.75 0.25 0.15 0.170 0.60 0.62 0.16 0.180 0.20 0.76 0.16 0.16 定位器训练配置文件cfg/yolo-origin.data 12345classes= 1 # 类的个数，本项目为1train = jiyan/data/train.txt # 存放train.txt文件的路径valid = jiyan/data/valid.txt # 存放valid.txt文件的路径names = /home/geng/gsxt_captcha/jiyan/data/yolo.names # 存放类名字的路径backup = jiyan/backup # 训练后权重保存位置 重要参数讲解： train：该参数为存放train.txt文件的路径，train.txt文件格式如下： 1234567............/home/geng/darknet/jiyan/data/train/1530946690.jpg/home/geng/darknet/jiyan/data/train/3062060956.jpg/home/geng/darknet/jiyan/data/train/3062062538.jpg............ 即train.txt文件保存了所有训练样本的路径。在上述路径中的train文件内必须同时存放样本及其对应的标签，即： 12345678............1528780328740.jpg 1530947539.jpg 3062055104.jpg 3062097132.jpg1528780328740.txt 1530947539.txt 3062055104.txt 3062097132.txt1528780333363.jpg 1530947544.jpg 3062055126.jpg 3062097142.jpg1528780333363.txt 1530947544.txt 3062055126.txt 3062097142.txt............ 生成train.txt：通过以下命令行命令来生成路径文件train.txt： 1find `pwd`/train -name \*.jpg &gt; train.txt valid：该参数为存放valid.txt文件的路径，valid.txt文件格式如下： 1234567............/home/geng/darknet/jiyan/data/valid/1530953866.jpg/home/geng/darknet/jiyan/data/valid/1530954184.jpg/home/geng/darknet/jiyan/data/valid/1530952279.jpg............ 即valid.txt文件保存了所有验证样本的路径。需要注意的是：上述路径中的valid文件内必须同时存放样本及其对应的.txt标签和.xml标签，即： 1234561530952510.jpg 1530953474.jpg 1530955039.jpg 3062099066.jpg 3062107032.jpg1530952510.txt 1530953474.txt 1530955039.txt 3062099066.txt 3062107032.txt1530952510.xml 1530953474.xml 1530955039.xml 3062099066.xml 3062107032.xml1530952529.jpg 1530953479.jpg 1530955044.jpg 3062099078.jpg 3062107044.jpg1530952529.txt 1530953479.txt 1530955044.txt 3062099078.txt 3062107044.txt1530952529.xml 1530953479.xml 1530955044.xml 3062099078.xml 3062107044.xml 生成valid.txt：通过以下命令行命令来生成路径文件valid.txt： 1find `pwd`/valid -name \*.jpg &gt; valid.txt 模型训练配置文件和数据集准备好之后，我们就可以开始训练了,训练命令如下: 1./darknet detector train cfg/yolo-origin.data cfg/yolo-origin.cfg 若在原有权重的基础上进行训练，使用如下命令： 1./darknet detector train cfg/yolo-origin.data cfg/yolo-origin.cfg jiyan/backup/yolo-origin.backup 训练结果评估目标检测中衡量识别精度的指标是mAP（mean average precision），mAP越接近1，表示定位效果越好。在计算mAP时，需要先根据训练的模型生成检测结果，然后使用getmap.py脚本计算mAP。 生成检测结果： 1./darknet detector valid cfg/yolo-origin.data cfg/yolo-origin.cfg jiyan/backup/yolo-origin.backup 生成的检测结果会存放在results/comp4_det_test_hanzi.txt文件内。 计算mAP： 1python getmap.py results/comp4_det_test_hanzi.txt jiyan/data/valid.txt hanzi 分类器训练训练数据准备样本的获取分类器样本的获取方法是通过对定位的汉字进行切割获取的。汉字切割脚本见python/segment.py。 样本的标注分类器样本的标注，即对切割的汉字图片进行标记。实质上是修改汉字图片的文件名，所涉及的原理在下面的文件准备中有讲解。标注过程解释如下： 汉字 汉字对应的Unicode 切割得到的汉字图片文件名 标记后的汉字图片文件名 健 \u5065 15310991303473_label.jpg 15310991303473_u5065.jpg 声 \u58f0 15310991261628_label.jpg 15310991261628_u58f0.jpg 汉字识别的标注脚本已经写好，见label_hanzi.py。 文件准备 分类器网络配置文件cfg/chinese_character.cfg 123456789101112131415161718192021222324252627282930313233343536373839404142434445[net]batch=64subdivisions=1height=64width=64channels=3max_crop=64min_crop=64angle=7hue=.1saturation=.75exposure=.75learning_rate=0.1policy=polypower=4max_batches = 45000momentum=0.9decay=0.0005[convolutional]batch_normalize=1filters=128size=3stride=1pad=1activation=leaky............[convolutional]filters=3604 # 此处需要与chinese_classify/data/labels.txt内的标签个数一致size=1stride=1pad=1activation=leaky[avgpool][softmax]groups=1[cost]type=sse 重要参数讲解： 最后一个convolutional层的filters：此处filters的值为3604，但是需要注意的是：此值应该与chinese_classify/data/labels.txt内的类标签个数一致。 分类器训练配置文件cfg/chinese.data 123456classes=3604 # 类的个数，此处即汉字的个数train = chinese_classify/data/train.list # 存放train.list文件的路径valid = chinese_classify/data/valid.list # 存放valid.list文件的路径labels = chinese_classify/data/labels.txt # 存放labels.txt的路径backup = chinese_classify/backup # 训练时权重的保存位置top=100 重要参数讲解： classes代表分类个数 train该参数为存放train.list文件的路径，train.list文件内格式如下： 1234567............/home/geng/darknet/chinese_classify/data/train/15310999153792_u5347.jpg/home/geng/darknet/chinese_classify/data/train/15310991401861_u4e58.jpg/home/geng/darknet/chinese_classify/data/train/15310993054970_u4e4b.jpg............ 即train.txt文件保存了所有训练样本的路径。在上述路径中的train文件内需要存放训练样本，即： 1234567............15310993303114_u5b9d.jpg 15310996251829_u5174.jpg 15310999168611_u6765.jpg15310993303400_u90e8.jpg 15310996252114_u4efb.jpg 15310999168899_u5145.jpg15310993303403_u5185.jpg 15310996252119_u4f55.jpg 15310999168901_u7535.jpg............ valid该参数的格式与train一样，不再赘述。只是该参数涉及的是验证集。 labels为存放类标签文件labels.txt的路径，labels.txt文件内格式如下： 123456789............u6bd3u5347u90f8u5d58u8426............ darknet是通过文件名与labels.txt中的字符串做匹配，匹配到则认为该标签为匹配到的字符串。如/home/geng/darknet/chinese_classify/data/train/15310999153792_u5347.jpg，由于labels.txt中出现u5347，所以这张图的标签为u5347。所以图片的路径绝对不可以出现多个labels.txt中包含的字符串，如果路径为somepath1/data2/3.jpg，labels.txt包含1,2,3，则这张图片会被认为匹配1,2,3多个label从而报错。 top代表valid时取前多少计算正确率。如top100代表分类时概率最大的前100类中出现了正确的标签就认为正确。 模型训练配置文件和数据集准备好之后，我们就可以开始训练了,训练命令如下: 1./darknet classifier train cfg/chinese.data cfg/chinese_character.cfg 若在原有权重的基础上进行训练，使用如下命令： 1./darknet classifier train cfg/chinese.data cfg/chinese_character.cfg chinese_classify/backup/chinese_character.backup 需要注意的是：如果增加样本后label.txt文件内增加了新的汉字标签，就不能在原有权重的基础上进行训练了，需要重新训练。 训练结果评估分类器模型验证比较简单，直接用准确率来评估，即：验证集中分类正确的个数/分类错误的个数。分类器模型验证命令如下： 1./darknet classifier valid cfg/chinese.data cfg/chinese_character.cfg chinese_classify/backup/chinese_character.backup 参考文献Darknet官方网站 You Only Look Once: Unified ,Real-Time Object Detection https://cos120.github.io/crack/ 免责声明该项目仅用于学术交流，不得任何商业使用！]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>yolo</tag>
        <tag>点击验证码</tag>
        <tag>国家企业信用信息公示系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[体验活动的生命周期]]></title>
    <url>%2F2018%2F11%2F16%2F%E4%BD%93%E9%AA%8C%E6%B4%BB%E5%8A%A8%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[在动手实现这小实验之前，最好先去了解活动生命周期的4种状态以及Activity类中定义的7个回调方法。 新建一个项目 新建一个名为ActivityLifeCycleTest的项目，点击Next。 保持默认选项，再点击Next。 选择一个空项目，让Android帮我们自动创建活动和布局。 活动名和布局名都是用默认值。 到此为止，我们的主活动就创建完毕了。 创建两个子活动创建两个子活动，分别名为NormalActivity和DialogActivity。 新建NormalActivity子活动。 布局起名为activity_normal，点击Finish。 新建DialogActivity子活动，布局起名为activity_dialog，创建方法同上。 到此，两个子活动创建完毕。 编写活动的布局文件 编写activity_normal.xml文件，将里面的代码替换成如下内容： 123456789101112&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"android:layout_width="match_parent"android:layout_height="match_parent"android:orientation="vertical"&gt;&lt;TextViewandroid:layout_width="match_parent"android:layout_height="wrap_content"android:text="This is a normal activity"/&gt;&lt;/LinearLayout&gt; 这个布局中我们使用了一个TextView，用于显示一行文字。 编辑activity_dialog.xml文件，将里面的内容代码替换成如下内容： 123456789101112&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"android:layout_width="match_parent"android:layout_height="match_parent"android:orientation="vertical"&gt;&lt;TextViewandroid:layout_width="match_parent"android:layout_height="wrap_content"android:text="This is a dialog activity"/&gt;&lt;/LinearLayout&gt; 修改AndroidManifest.xml文件的&lt;activity&gt;标签的配置，将DialogActivity活动指定为对话框式的主题。 1234567891011&lt;activity android:name=".MainActivity"&gt;&lt;intent-filter&gt;&lt;action android:name="android.intent.action.MAIN" /&gt;&lt;category android:name="android.intent.category.LAUNCHER" /&gt;&lt;/intent-filter&gt;&lt;/activity&gt;&lt;activity android:name=".NormalActivit" /&gt;&lt;activity android:name=".DialogActivity"android:theme="@style/Theme.AppCompat.Dialog"&gt;&lt;/activity&gt; 修改activity_main.xml文件，重新定制主活动的布局，替换为如下内容： 12345678910111213141516171819&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"android:layout_width="match_parent"android:layout_height="match_parent"android:orientation="vertical"&gt;&lt;Buttonandroid:id="@+id/start_normal_activity"android:layout_width="match_parent"android:layout_height="wrap_content" android:text="Start NormalActivity"/&gt;&lt;Buttonandroid:id="@+id/start_dialog_activity"android:layout_width="match_parent"android:layout_height="wrap_content"android:text="Start DialogActivity"/&gt;&lt;/LinearLayout&gt; 我们在LineaLayout中加入了两个按钮，一个用于启动NormalActivity，一个用于启动DialogActivity。 修改MainActivity123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package cn.edu.pku.gengzehao.activitylifecycletest;import android.content.Intent;import android.support.v7.app.AppCompatActivity;import android.os.Bundle;import android.util.Log;import android.view.View;import android.widget.Button;public class MainActivity extends AppCompatActivity implements View.OnClickListener &#123;public static final String TAG = "MainActivity";@Overrideprotected void onCreate(Bundle savedInstanceState) &#123;super.onCreate(savedInstanceState);Log.d(TAG, "onCreate");setContentView(R.layout.activity_main);Button startNormalActivity = (Button) findViewById(R.id.start_normal_activity);startNormalActivity.setOnClickListener(this);Button startDialogActivity = (Button) findViewById(R.id.start_dialog_activity);startDialogActivity.setOnClickListener(this);&#125;@Overridepublic void onClick(View v) &#123;if (v.getId() == R.id.start_normal_activity)&#123;Intent intent = new Intent(MainActivity.this, NormalActivit.class);startActivity(intent);&#125;if (v.getId() == R.id.start_dialog_activity)&#123;Intent intent = new Intent(MainActivity.this, DialogActivity.class);startActivity(intent);&#125;&#125;@Overrideprotected void onStart()&#123;super.onStart();Log.d(TAG, "onStart");&#125;@Overrideprotected void onResume()&#123;super.onResume();Log.d(TAG, "onResume");&#125;@Overrideprotected void onPause()&#123;super.onPause();Log.d(TAG, "onPause");&#125;@Overrideprotected void onStop()&#123;super.onStop();Log.d(TAG, "onStop");&#125;@Overrideprotected void onDestroy()&#123;super.onDestroy();Log.d(TAG, "onDestroy");&#125;@Overrideprotected void onRestart()&#123;super.onRestart();Log.d(TAG, "onRestart");&#125;&#125; 再onCreate方法中，我们我们分别为两个按钮注册了点击事件，点击第一个按钮会启动NoramalActivity。点击第二个按钮会启动DialogActivity。然后再Activity的7个回调方法中分别打印了回调方法的名字，这样就可以通过观察日志的方式来更直观的理解活动的生命周期。 运行程序 启动程序： 启动程序时，logcat中的日志信息如下图，可以看到当MainActivity第一被创建时会一次执行onCreate()、onStart()、onResume()方法。 点击一个按钮，启动NormalActivity。 打开NormalActivity时的打印日志： 由于NormalActivity已经把MainActivity完全遮挡，因此onPause()和onStop()方法都会得到执行。 按下Back键返回MainActivity的打印日志信息如下。 由于之前的MainActivity已经进入了停止状态，所以onRestart() 方法才会的到执行，之后又会依次执行onSrart() 和onResume()方法。值的注意的是，此时onCreate()方法不会执行。 再点击第二个按钮，启动DialogActivity。 打开DialogActivity时的打印日志： 只有onPause()方法得到了执行，onStop()方法并没有执行。这是因为DialogActivity并没有完全遮挡住MainActivity，此时MainActivity只是进入了暂停状态，并没有进入停止状态。 按下Back键再次返回MainActivity时的打印日志为： 可以看到，只有onResume ()方法得到了执行。 最后，在MainActivity按下Back键退出程序，打印日志如下： 依次会执行onPause()、onStop()、 onDestroy()方法，最终销毁MainActivity。 参考文献《第一行代码 Android》郭霖]]></content>
      <categories>
        <category>Android</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客迁移到一台新电脑]]></title>
    <url>%2F2018%2F10%2F29%2FHexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%8F%A6%E4%B8%80%E5%8F%B0%E7%94%B5%E8%84%91%2F</url>
    <content type="text"><![CDATA[前言由于10月份换了一台Macbook Pro，导致自己搭建的Hexo博客一直停滞了。我想人做事一定要善始善终，不要忘记当初搭建Hexo博客的初心。于是乎，就有了这一篇文章。 迁移思路在已经推送到Github上的Hexo静态网页创建一个分支，利用这个分支来管理自己的Hexo环境文件。 迁移步骤1.在旧机器上克隆Github上面生成的静态文件到本地1git clone https://github.com/username/username.github.io.git 2.针对克隆到本地的文件中，将除去.git文件的所有文件都删除3.将旧机器中所有文件(.gitignore文件中包含的文件除外）拷贝到我们克隆下来的文件内4. 创建并切换到一个叫hexo的分支1git checkout -b hexo 5. 提交复制过来的文件到暂存取1git add -A 6.提交1git commit -m "ceate a new branch file" 7.推送到Github1git push --set-upstream origin hexo 这个时候hexo分支已经创建完毕，接下来，我们在新电脑上搭建环境。 8.新电脑配置环境安装node.js，根据自己电脑系统自行百度安装。 安装git，git相关教程推荐廖雪峰老师的git教程。 安装hexo： npm install -g hexo-cli 9.clone远程仓库到本地1git clone -b hexo https://github.com/username/username.github.io.git 10.根据package.json安装依赖1npm install *** --save 将***替换为package.json文件内的依赖包 11.开始写文章我们现在可以通过hexo n &quot;文章标题&quot; 创建一篇文章了！ 12. 提交hexo环境文件git add . git commit -m &quot;some description&quot; git push origin hexo 13.发布文章1hexo g -d 到这里，我们的Hexo博客就迁移完毕了！！以后再写文章时，只需要重复步骤11、12、13就ok啦！！]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Scrapy框架的CrawlSpider类爬取当当全网商品信息]]></title>
    <url>%2F2018%2F09%2F01%2F%E5%9F%BA%E4%BA%8EScrapy%E6%A1%86%E6%9E%B6%E7%9A%84CrawlSpider%E7%B1%BB%E7%88%AC%E5%8F%96%E5%BD%93%E5%BD%93%E5%85%A8%E7%BD%91%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言本项目通过使用Scrapy框架的CrawlSpider类，对当当全网商品信息进行爬取并将信息保存至mysql数据库，当当网反爬措施是对IP访问频率的限制，所以本项目使用了中间件scrapy-rotating-proxies来管控IP代理池，有关代理ip的爬取请见我的另一篇博文。 CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类通过定义一些规则(rule)来跟进所爬取网页中的link，从爬取的网页中获取link并继续爬取。 Github地址: https://github.com/RunningGump/crawl_dangdang 依赖 scrapy 1.5.0 python3.6 mysql 5.7 pymysql 库 scrapy-rotating-proxies 库 fake-useragent 库 创建项目首先，我们需要创建一个Scrapy项目，在shell中使用scrapy startproject命令： 1234567$ scrapy startproject DangdangNew Scrapy project 'Dangdang', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in: /home/geng/DangdangYou can start your first spider with: cd Dangdang scrapy genspider example example.com 创建好一个名为Dangdang的项目后，接下来，你进入新建的项目目录： 1$ cd Dangdang 然后,使用scrapy genspider -t &lt;template&gt; &lt;name&gt; &lt;domain&gt;创建一个spider： 123$ scrapy genspider -t crawl dd dangdang.comCreated spider 'dd' using template 'crawl' in module: Dangdang.spiders.dd 此时，你通过cd ..返回上级目录，使用tree命令查看项目目录下的文件，显示如下： 1234567891011121314151617181920$ cd ..$ tree Dangdang/Dangdang/├── Dangdang│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── __pycache__│ │ ├── __init__.cpython-36.pyc│ │ └── settings.cpython-36.pyc│ ├── settings.py│ └── spiders│ ├── dd.py│ ├── __init__.py│ └── __pycache__│ └── __init__.cpython-36.pyc└── scrapy.cfg4 directories, 11 files 到此为止，我们的项目就创建成功了。 rules在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作设置了爬取规则。 参数介绍： link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。 callback： 回调函数，对link_extractor获得的链接进行处理与解析。 注意事项：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。 follow：是一个布尔(boolean)值，指定了根据规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤链接。 process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request) LinkExtrator参数介绍： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)匹配的URL不提取。 allow_domains：会被提取的链接的域名。 deny_domains：不会被提取链接的域名。 restrict_xpaths：使用Xpath表达式与allow共同作用提取出同时符合对应Xpath表达式和正则表达式的链接； 项目代码编写item.py文件123456789101112import scrapyclass DangdangItem(scrapy.Item): category = scrapy.Field() # 商品类别 title = scrapy.Field() # 商品名称 link = scrapy.Field() # 商品链接 price = scrapy.Field() # 商品价格 comment = scrapy.Field() # 商品评论数 rate = scrapy.Field() # 商品的好评率 source = scrapy.Field() # 商品的来源地 detail = scrapy.Field() # 商品详情 img_link = scrapy.Field() # 商品图片链接 编写pipeline.py文件需提前创建好数据库，本项目创建的数据库名字为dd。 123456789101112131415161718192021222324252627282930313233343536373839import pymysqlfrom scrapy.conf import settings## pipeline默认是不开启的，需在settings.py中开启class DangdangPipeline(object): def process_item(self, item, spider): ##建立数据库连接 conn = pymysql.connect(host="localhost",user="root",passwd='yourpasswd',db="dd",use_unicode=True, charset="utf8") cur = conn.cursor() # 用来获得python执行Mysql命令的方法,也就是我们所说的操作游标 print("mysql connect success") # 测试语句，这在程序执行时非常有效的理解程序是否执行到这一步 try: category = item["category"] title = item["title"] if len(title)&gt;40: title = title[0:40] + '...' link = item["link"] img_link = item['img_link'] price = item["price"] comment = item["comment"] rate = item["rate"] source = item["source"] detail = item["detail"] sql = "INSERT INTO goods(category,title,price,comment,rate,source,detail,link,img_link) VALUES ('%s','%s','%s','%s','%s','%s','%s','%s','%s')" % (category,title,price,comment,rate,source,detail,link,img_link) print(sql) except Exception as err: pass try: cur.execute(sql) # 真正执行MySQL语句，即查询TABLE_PARAMS表的数据 print("insert success") # 测试语句 except Exception as err: print(err) conn.rollback() #事务回滚,为了保证数据的有效性将数据恢复到本次操作之前的状态.有时候会存在一个事务包含多个操作，而多个操作又都有顺序，顺序执行操作时，有一个执行失败，则之前操作成功的也会回滚，即未操作的状态 else: conn.commit() #当没有发生异常时，提交事务，避免出现一些不必要的错误 conn.close() #关闭连接 return item #框架要求返回一个item对象 编写middlewares.py本项目添加了RandomUserAgentMiddleWare中间件，用来随机更换UserAgent。在middlewares.py文件的最后面添加如下中间件： 1234567891011121314151617181920from fake_useragent import UserAgentclass RandomUserAgentMiddleWare(object): """ 随机更换User-Agent """ def __init__(self,crawler): super(RandomUserAgentMiddleWare, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get("RANDOM_UA_TYPE", "random") @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua_type(): return getattr(self.ua, self.ua_type) # 取对象 ua 的 ua_type 的这个属性, 相当于 self.ua.self.ua_type request.headers.setdefault('User-Agent', get_ua_type()) 修改settings.py 文件12345678910111213141516171819202122232425262728293031BOT_NAME = 'Dangdang'SPIDER_MODULES = ['Dangdang.spiders']NEWSPIDER_MODULE = 'Dangdang.spiders'# 不遵循robots协议ROBOTSTXT_OBEY = False# 下载延迟设置为0，提高爬取速度DOWNLOAD_DELAY = 0#禁用Cookie(默认情况下启用)COOKIES_ENABLED = False # 启用所需要的下载中间件DOWNLOADER_MIDDLEWARES = &#123; 'rotating_proxies.middlewares.RotatingProxyMiddleware': 610, 'rotating_proxies.middlewares.BanDetectionMiddleware': 620, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'Dangdang.middlewares.RandomUserAgentMiddleWare': 400,&#125;# 代理IP文件路径,此处需改为你自己的路径ROTATING_PROXY_LIST_PATH = '/home/geng/Projects/Dangdang/proxy.txt'# 随机更换UserAgent RANDOM_UA_TYPE = "random"# 开启pipelineITEM_PIPELINES = &#123; 'Dangdang.pipelines.DangdangPipeline': 300,&#125; spider文件(dd.py)编写12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom Dangdang.items import DangdangItemimport reimport urllib.requestimport jsonclass DdSpider(CrawlSpider): name = 'dd' allowed_domains = ['dangdang.com'] start_urls = ['http://category.dangdang.com/'] # 分析网页链接，编写rules规则,提取商品详情页的链接 rules = ( Rule(LinkExtractor(allow=r'/cp\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html$|/pg\d+-cp\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html$', deny=r'/cp98.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html'), follow=True), Rule(LinkExtractor(allow=r'/cid\d+.html$|/pg\d+-cid\d+.html$', deny=r'/cp98.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.\d&#123;2&#125;.html'), follow=True), Rule(LinkExtractor(allow=r'product.dangdang.com/\d+.html$', restrict_xpaths=("//p[@class='name']/a")), callback='parse_item', follow=False), # allow与restrict_xpath配合使用,效果很好,可以更精准筛选链接. ) # 解析商品详情页面 def parse_item(self, response): item = DangdangItem() # 实例化item item["category"] = response.xpath('//*[@id="breadcrumb"]/a[1]/b/text()').extract_first()+'&gt;'+response.xpath('//*[@id="breadcrumb"]/a[2]/text()').extract_first()+'&gt;'+response.xpath('//*[@id="breadcrumb"]/a[3]/text()').extract_first() item["title"] = response.xpath("//*[@id='product_info']/div[1]/h1/@title").extract_first() item["detail"] = json.dumps(response.xpath("//*[@id='detail_describe']/ul//li/text()").extract(),ensure_ascii=False) item["link"] = response.url item["img_link"] =json.dumps(response.xpath("//div[@class='img_list']/ul//li/a/@data-imghref").extract()) try: item["price"] = response.xpath("//*[@id='dd-price']/text()").extract()[1].strip() except IndexError as e: item["price"] = response.xpath("//*[@id='dd-price']/text()").extract()[0].strip() item["comment"] = response.xpath("//*[@id='comm_num_down']/text()").extract()[0] try: item["source"] = response.xpath("//*[@id='shop-geo-name']/text()").extract()[0].replace('\xa0至','') except IndexError as e: item["source"] = '当当自营' # 通过抓包分析,提取商品的好评率 goodsid = re.compile('\/(\d+).html').findall(response.url)[0] # 提取url中的商品id # 提取详情页源码中的categoryPath script = response.xpath("/html/body/script[1]/text()").extract()[0] categoryPath = re.compile(r'.*categoryPath":"(.*?)","describeMap').findall(script)[0] # 构造包含好评率包的链接 rate_url = "http://product.dangdang.com/index.php?r=comment%2Flist&amp;productId="+str(goodsid)+"&amp;categoryPath="+str(categoryPath)+"&amp;mainProductId="+str(goodsid) rate_date = urllib.request.urlopen(rate_url).read().decode("utf-8") # 读取包含好评率的包的内容 item["rate"] = re.compile(r'"goodRate":"(.*?)"').findall(rate_date)[0]+'%' # 用正则表达式提取好评率 yield item 使用方法在命令行中执行以下命令开始爬取： 1$ scrapy crawl dd 结果展示]]></content>
      <categories>
        <category>python3网络爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于scrapy框架爬取西刺代理并验证有效性]]></title>
    <url>%2F2018%2F08%2F02%2F%E5%9F%BA%E4%BA%8Escrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E8%A5%BF%E5%88%BA%E4%BB%A3%E7%90%86%E5%B9%B6%E9%AA%8C%E8%AF%81%E6%9C%89%E6%95%88%E6%80%A7%2F</url>
    <content type="text"><![CDATA[前言在爬取网站数据的时候，一些网站会对用户的访问频率进行限制，如果爬取过快会被封ip，而使用代理可防止被封禁。本项目使用scrapy框架对西刺网站进行爬取，并验证爬取代理的有效性，最终将有效的代理输出并存储到json文件中。 Github地址: https://github.com/RunningGump/crawl_xiciproxy 依赖 python3.6 Scrapy 1.5.0 创建项目首先，我们需要创建一个Scrapy项目，在shell中使用scrapy startproject命令： 1234567$ scrapy startproject xiciproxyNew Scrapy project 'xiciproxy', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in: /home/geng/xiciproxyYou can start your first spider with: cd xiciproxy scrapy genspider example example.com 创建好一个名为xiciproject的项目后，接下来，你进入新建的项目目录： 1$ cd xiciproxy 然后,使用scrapy genspider &lt;name&gt; &lt;domain&gt;创建一个spider： 123$ scrapy genspider xici xicidaili.comCreated spider 'xici' using template 'basic' in module: xiciproxy.spiders.xici 此时，你通过cd ..返回上级目录，使用tree命令查看项目目录下的文件，显示如下： 1234567891011121314151617181920$ cd ..$ tree xiciproxyxiciproxy├── scrapy.cfg└── xiciproxy ├── __init__.py ├── items.py ├── middlewares.py ├── pipelines.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── settings.cpython-36.pyc ├── settings.py └── spiders ├── __init__.py ├── __pycache__ │ └── __init__.cpython-36.pyc └── xici.py4 directories, 11 files 到此为止，我们的项目就创建成功了。 分析页面编写爬虫程序之间，首先需要对待爬取的页面进行分析，主流的浏览器中都带有分析页面的工具或插件，这里我们选用Chrome浏览器的开发者工具分析页面。 链接信息在Chrome浏览器中打开页面http://www.xicidaili.com/, 通过点击国内高匿代理和国内普通代理以及进行翻页操作，会发现以下规律： http://www.xicidaili.com/参数1/参数2 参数1中nn代表高匿代理，nt代表普通代理；参数2中1,2,3,4…代表页数。 数据信息爬取网页信息时一般使用高匿代理，高匿代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真是IP是隐藏的，不会认为我们使用了代理。 本部分以爬取高匿代理为例子来分析如何爬取网页的数据信息。在Chrome浏览器中打开页面http://www.xicidaili.com/nn, 并按F12键来打开开发者工具，点击Elements（元素）来查看其HTML代码，会发现每一条代理的信息都包裹在一个tr标签下，如下图所示： 再来单独对一个tr标签进行分析： 123456789101112131415161718192021222324252627&lt;tr class="odd"&gt; &lt;td class="country"&gt;&lt;img src="http://fs.xicidaili.com/images/flag/cn.png" alt="Cn"&gt;&lt;/td&gt; &lt;td&gt;115.198.35.213&lt;/td&gt; &lt;td&gt;6666&lt;/td&gt; &lt;td&gt; &lt;a href="/2018-07-20/zhejiang"&gt;浙江杭州&lt;/a&gt; &lt;/td&gt; &lt;td class="country"&gt;高匿&lt;/td&gt; &lt;td&gt;HTTPS&lt;/td&gt; &lt;td class="country"&gt; &lt;div title="0.144秒" class="bar"&gt; &lt;div class="bar_inner fast" style="width:85%"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td class="country"&gt; &lt;div title="0.028秒" class="bar"&gt; &lt;div class="bar_inner fast" style="width:96%"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td&gt;15天&lt;/td&gt; &lt;td&gt;18-08-04 15:33&lt;/td&gt; &lt;/tr&gt; 会发现：IP地址包裹在td[2]标签下，端口port包裹在td[3]标签下，类型（http/https）包裹在td[6]标签下。 程序编写分析完页面后，接下来编写爬虫。本项目主要是对xici.py进行编写，对settings.py仅做了轻微改动。 实现spider即编写xici.py文件，程序如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import scrapyimport json'''scrapy crawl xici -o out.json -a num_pages=50 -a typ=nn其中`out.json`是输出有效代理的json文件，`num_pages`是爬取页数，`typ`表示代理类型，`nn`是高匿代理，`nt`是普通代理。'''class XiCiSpider(scrapy.Spider): # 每一个爬虫的唯一标识 name = 'xici' # 使用-a选项,可以将命令行参数传递给spider的__init__方法 def __init__(self, num_pages=5, typ='nn', *args, **kwargs): num_pages = int(num_pages) self.num_pages = num_pages self.typ = typ # 定义起始爬取点 def start_requests(self): for page in range(1, self.num_pages + 1): url = 'http://www.xicidaili.com/&#123;&#125;/&#123;&#125;'.format(self.typ, page) yield scrapy.Request(url=url) # 解析response返回的网页 def parse(self, response): proxy_list = response.xpath('//table[@id = "ip_list"]/tr[position()&gt;1]') for tr in proxy_list: # 提取代理的 ip, port, scheme(http or https) ip = tr.xpath('td[2]/text()').extract_first() port = tr.xpath('td[3]/text()').extract_first() scheme = tr.xpath('td[6]/text()').extract_first() # 使用爬取到的代理再次发送请求到http(s)://httpbin.org/ip, 验证代理是否可用 url = '%s://httpbin.org/ip' % scheme proxy = '%s://%s:%s' % (scheme, ip, port) meta = &#123; 'proxy': proxy, 'dont_retry': True, 'download_timeout': 5, # 下面的ip字段是传递给check_available方法的信息,方便检测是否可隐藏ip '_proxy_ip':ip, &#125; yield scrapy.Request(url, callback=self.check_available, meta=meta, dont_filter=True) def check_available(self, response): proxy_ip = response.meta['_proxy_ip'] # 判断代理是否具有隐藏IP功能 if proxy_ip == json.loads(response.text)['origin']: yield&#123; 'proxy': response.meta['proxy'] &#125; 修改配置文件 更改USER_AGENT：西刺代理网站会通过识别请求中的user-agent来判断这次请求是真实用户所为还是机器所为。 不遵守robots协议：网站会通过robots协议告诉搜索引擎那些页面可以抓取，哪些不可以抓取，而robots协议大多不允许抓取有价值的信息，所以咱们不遵守。 禁用cookies：如果用不到cookies，就不要让服务器知道你的cookies。 文件settings.py中的改动如下： 123USER_AGENT = &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36&quot;ROBOTSTXT_OBEY = TrueCOOKIES_ENABLED = False 在编写好xici.py和settings.py后，我们的项目就大功告成啦！ 使用方法使用方法就是在命令行中执行以下命令即可： 1$ scrapy crawl xici -o out.json -a num_pages=10 -a typ=nn 其中out.json是最终输出有效代理的json文件，num_pages是爬取页数，typ表示要爬取的代理类型，nn是高匿代理，nt是普通代理。 提示 ：程序在验证代理有效性的过程中，对于无效的代理会抛出超时异常，不要管这些异常，让程序继续执行直到结束。]]></content>
      <categories>
        <category>python3网络爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>西刺代理</tag>
      </tags>
  </entry>
</search>
